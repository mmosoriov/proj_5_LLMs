{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmosoriov/proj_5_LLMs/blob/main/Part2_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Name: Miguel Mateo Osorio Vela, Matthew West\n",
        "\n",
        "Development environment: Colab"
      ],
      "metadata": {
        "id": "gz8WVLBnGrhm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Programmatically Prompting LLMs for Tasks\n",
        "\n",
        "- **Tasks:**\n",
        "  1. Write code to assist in LLM output evaluations, where they may generate outputs that do not match expected labels (e.g., generating \"Maybe\" to a Yes or No question). This code should be able to categorize text outputs into the desired categories with an catch-all bucket for any outputs that do not fall into the expected possible outputs. Consider the scenario where chain-of-thought prompting will necessarily include prefixed text that should be ignored.\n",
        "  2. Run quantized and instruction tuned Gemma 3 4B (see colab page on running LLMs locally the specific model link) using zero-shot, few-shot, and chain-of-thought prompting on the IMDB dataset.\n",
        "  3. Compare LLM results against both the RNN and simple baseline.\n",
        "  4. Discuss the observed results.\n",
        "\n",
        "_Where it is relevant, make sure you follow deep learning best practices discussed in class. In particular, performing a hyperparameter search and setting up an proper train, dev, and test framework for evaluating hyperparameters and your final selected model._\n",
        "\n",
        "- Evaluation scenarios:\n",
        "\n",
        "  **Review Text Classification**\n",
        "    - Use 2,000 examples for training (if needed) and 100 examples for testing (much smaller than deep learning because LLMs on CPU only are *very* slow).\n",
        "    - Use zero-shot, few-shot (4 examples - 2 good, 2 bad), and chain-of-thought prompting\n",
        "    - Ensure that prompts are formatted to give the LLM a good shot at succeeding (properly format Gemma 3 instructions and include appropriate system messages)\n",
        "    - Plot a confusion matrix of the predictions.\n",
        "\n",
        "- Discussion:  \n",
        "  - Which setting of LLMs performs the best?\n",
        "  - Which approach performs the best overall?\n",
        "  - How much does LLM performance vary by prompting strategy?\n",
        "  - What are the benefits and drawbacks of using LLMs for classification tasks such as movie review classification? *Cite specific evidence from this project.*"
      ],
      "metadata": {
        "id": "eWg1Qq2KHbEP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMDB Movie Review Dataset\n",
        "Description from https://www.tensorflow.org/datasets/catalog/imdb_reviews:\n",
        "> Large Movie Review Dataset. This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well."
      ],
      "metadata": {
        "id": "sg3_dalVNwPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "YOO3rQyRNtop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load dataset"
      ],
      "metadata": {
        "id": "j6OBAPrlOQA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset, info = tensorflow_datasets.load('imdb_reviews', with_info=True, as_supervised=True)\n",
        "train_dataset, test_dataset = dataset['train'], dataset['test']"
      ],
      "metadata": {
        "id": "gnrMaWrvONWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get subset of the data for training and testing (2000 samples each). Convert Keras dataset to lists of strings and labels."
      ],
      "metadata": {
        "id": "nn6foYqeOT-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = []\n",
        "y_train = []\n",
        "\n",
        "for sample, label in train_dataset.take(2000):\n",
        "  x_train.append(sample.numpy())\n",
        "  y_train.append(label.numpy())\n",
        "\n",
        "x_train = np.asarray(x_train)\n",
        "y_train = np.asarray(y_train)\n",
        "\n",
        "print(x_train[0])\n",
        "print(y_train[0])\n",
        "\n",
        "x_test = []\n",
        "y_test = []\n",
        "\n",
        "for sample, label in test_dataset.take(100):\n",
        "  x_test.append(sample.numpy())\n",
        "  y_test.append(label.numpy())\n",
        "\n",
        "x_test = np.asarray(x_test)\n",
        "y_test = np.asarray(y_test)\n",
        "\n",
        "print(x_test[0])\n",
        "print(y_test[0])"
      ],
      "metadata": {
        "id": "a1iCXfbCOR-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Add your comparisons (baseline + RNN)\n",
        "\n",
        "Here is the code for my comparison models from the deep learning part of the project."
      ],
      "metadata": {
        "id": "nN9FUZ6eOmB0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the experiments using Gemma and comparisons\n",
        "\n",
        "Here is the code I used to get the results below! Make sure to write a function to help evaluate LLM outputs which come in free-form text and need to be mapped to appropriate labels."
      ],
      "metadata": {
        "id": "xxR7SQXnOthj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xVINIrwx4zDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Report your results\n",
        "\n",
        "Check these amazing plots & discussion."
      ],
      "metadata": {
        "id": "M2oy0mbcOuTr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i1vuAbOC4dew"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}