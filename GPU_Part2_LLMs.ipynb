{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Name: Miguel Mateo Osorio Vela, Matthew West\n",
        "\n",
        "Development environment: Colab"
      ],
      "metadata": {
        "id": "gz8WVLBnGrhm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Programmatically Prompting LLMs for Tasks\n",
        "\n",
        "- **Tasks:**\n",
        "  1. Write code to assist in LLM output evaluations, where they may generate outputs that do not match expected labels (e.g., generating \"Maybe\" to a Yes or No question). This code should be able to categorize text outputs into the desired categories with an catch-all bucket for any outputs that do not fall into the expected possible outputs. Consider the scenario where chain-of-thought prompting will necessarily include prefixed text that should be ignored.\n",
        "  2. Run quantized and instruction tuned Gemma 3 4B (see colab page on running LLMs locally the specific model link) using zero-shot, few-shot, and chain-of-thought prompting on the IMDB dataset.\n",
        "  3. Compare LLM results against both the RNN and simple baseline.\n",
        "  4. Discuss the observed results.\n",
        "\n",
        "_Where it is relevant, make sure you follow deep learning best practices discussed in class. In particular, performing a hyperparameter search and setting up an proper train, dev, and test framework for evaluating hyperparameters and your final selected model._\n",
        "\n",
        "- Evaluation scenarios:\n",
        "\n",
        "  **Review Text Classification**\n",
        "    - Use 2,000 examples for training (if needed) and 100 examples for testing (much smaller than deep learning because LLMs on CPU only are *very* slow).\n",
        "    - Use zero-shot, few-shot (4 examples - 2 good, 2 bad), and chain-of-thought prompting\n",
        "    - Ensure that prompts are formatted to give the LLM a good shot at succeeding (properly format Gemma 3 instructions and include appropriate system messages)\n",
        "    - Plot a confusion matrix of the predictions.\n",
        "\n",
        "- Discussion:  \n",
        "  - Which setting of LLMs performs the best?\n",
        "  - Which approach performs the best overall?\n",
        "  - How much does LLM performance vary by prompting strategy?\n",
        "  - What are the benefits and drawbacks of using LLMs for classification tasks such as movie review classification? *Cite specific evidence from this project.*"
      ],
      "metadata": {
        "id": "eWg1Qq2KHbEP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMDB Movie Review Dataset\n",
        "Description from https://www.tensorflow.org/datasets/catalog/imdb_reviews:\n",
        "> Large Movie Review Dataset. This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well."
      ],
      "metadata": {
        "id": "sg3_dalVNwPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "YOO3rQyRNtop"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load dataset"
      ],
      "metadata": {
        "id": "j6OBAPrlOQA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset, info = tensorflow_datasets.load('imdb_reviews', with_info=True, as_supervised=True)\n",
        "train_dataset, test_dataset = dataset['train'], dataset['test']"
      ],
      "metadata": {
        "id": "gnrMaWrvONWS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155,
          "referenced_widgets": [
            "ac67b237dfc64d47a1ed4ddfdf9b81e1",
            "19206d6ab8a641b486b097b2ae2951bf",
            "035b15fbfac44c3f839aba756e3356e6",
            "39760c63e96747e8a49ede54e3a12e6e",
            "41e8586bc7dc465c9043928639c4a1b1",
            "894d74fcc6e940fd8e016878e6fd21c2",
            "b387e150d70543639fa698ac29868007",
            "4a8d0c70900a4edbb752138ade24e756",
            "d9790afe13f6442d9756857b3acd5c21",
            "1e82e8d407624d638873891f9fb81025",
            "c90fdcf395e14da28a10a7ef1f9db37a",
            "13f74510b4ce4e51ba9be21495469339",
            "35d9fd2c1d34425f80bb2fb31ef01663",
            "7906024d457844d0aa368c41f224fb0e",
            "99796fbbfd1c4b0b8dd35565866b9c5a",
            "62e694e796f34d909a1cadeed26f5477",
            "49f8bfd0af524bf1bae395ba7ea4cf61",
            "bf05ac3f88c248778be257787ac65298",
            "55cf322110974d8fb381ea7a26d70f48",
            "6ae557ba2b9a4a51b64e3c7753562eb2",
            "fde5dc27fb0741dc815ad4023bcd8ab1",
            "e83d676748514892b1ccb06dc9237384",
            "65636a4f02e245cfb00cb55112572fdd",
            "fd669f1ea031413a95cd102c3ce5e28b",
            "d6fd8e74e1724089baa23ad4509bbca7",
            "346116eea4284811bd41fe9ce7abab57",
            "784d39a0cd964d54b454a90d78b9d023",
            "d17182f4d0c144e4836af9e24e5be8f6",
            "c4aaf53011d146129043a9388c74aa3e",
            "40c0324257f5460e9c3534c774d3a399",
            "5e505e8588ac4d35bfc77143098c1287",
            "86d35113f0d64753a565604e18ebbd79",
            "b8add231d20544c387cb471418af9ff6",
            "98d101cd87b641e49ecb3c5398759a3a",
            "cdf7b0c33b074ce28307cfe7ee9fa460",
            "2d2580c340d34674abb52f14e56c4319",
            "a5021b0d5967463a91637c60d63beee3",
            "6b68d2f9e569450abd978d09f154f6ed",
            "3c45626b81de454d9966312c1d3c0efa",
            "99af2ad766924517b9a7b8c7ce7d283b",
            "7ce8f7d6f566417abf36a83dc4cdadbf",
            "e621c34f3dd94e64a707e9f93ab09859",
            "ad00b999f192415caef1f35d7c9fc0df",
            "f26c7e84a2e541e895a9e5ebe4aa2749",
            "653d007237fd46379e6eca16778aef78",
            "b2421bfab8804e5387886b565d7c7c08",
            "c66bdbe5365f4b2ebf18a9923b3c71f3",
            "3423425396604767b5d4c66767e772da",
            "ec3897c630294a0aad98470f0f6c854c",
            "28ac16d8bf3f4bd3a90cf69c5afb5128",
            "fef05e9db50140a3b5d70e3088105d14",
            "caf31d3941814442b1038dc5c878f784",
            "d3d455c959df4183a96480a4d2e201a3",
            "a9d17981644f428286f5577b09dad12d",
            "13a0bd6b38ec48b7bd2697f5af5c4fc5",
            "76ddf84428b446d094d2440901a0530b",
            "047de900ab7e43d8a101a9d50d063605",
            "26820815451b4441a884ed5b54e4170b",
            "6a432073393e4db89ee137d8b0585918",
            "d95a6aabe5d94ddebb6ca8ee9fd56bd3",
            "d6c19a00c00341be89561c71338384a1",
            "59ac07eab39c47d4a9f75a1877bd317a",
            "d06b8730bb494f7699896a8d0be0ace5",
            "f9aa7e25d13c4210a6af6f0d1ee6ef4c",
            "8f5588cdd626404e9b5c59acfd5dea7c",
            "409642794e06406f85422740eb1c5301",
            "6fd1717c38d0419bb79d26fd82b33b64",
            "87b379b3aba04df0b3666a8155045879",
            "a4003bfb665c4cf7a51c74b38be0ee36",
            "34139a2ef1b7469f8af61557489c3b07",
            "7873824617b7465d93a57e6d1fb56aa8",
            "3eae7bdf8289433b890f73dd370b760c",
            "1b548860a3284c0db8cccbfe7acbb37a",
            "fb6c32bda11c4d82a496c56447a1dbf4",
            "4310be3213714c89a0e71852f329dc42",
            "a0bdcd0c70cf40bbb74291984f8cf191",
            "f69b7a0e59814534867ec065932c2b4d",
            "c4e08ee398ad4251ac86510143ac9b20",
            "cb73d16e4a2a4676b506e7722e548406",
            "ae846ba0bf4b4ab8aa558fc6d517a26b",
            "63882a3937ec4c77bab7e7a2b9e4d66b",
            "dff68ac529794160b0f88ebea1d0ee35",
            "d101266ae1604eccbc35bce04f90a31c",
            "c56fcc33d6114d4cb944920a28074def",
            "41be36e4e4c54360953e38c88c5c6c95",
            "f16f347ab22742089bda11da6e3b33c6",
            "1aa9351ded24437e928c5423f5f2d613",
            "ce0bbbdb5f0a464598cbe1ac6b123867",
            "2b1130bbd3ff4abd895ea4ba469bb0c4",
            "9fff2417ed6a4b71a4193894c9b3587b",
            "c4e4bf787664492d9331ccc44d323080",
            "68d9a1588052421280aceadda5ea0018",
            "2513198196e14d0eb075881de79c6460",
            "9ff41fd13db14c0bbb033b31ee177588",
            "4bc6e6c992f14e40adee35870154e94c",
            "0c3e00a5e58a47d0be6d7a9c4f05504e",
            "2d22936fcd1641c1a75cf82b7bbe5f0e",
            "bff0c797a4254b80a1e28cd4677c5611",
            "56120a4debae4326b0d6f1e83724658e"
          ]
        },
        "outputId": "2c63afc0-9280-442d-b422-92fa283283ef"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Variant folder /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0 has no dataset_info.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dl Completed...: 0 url [00:00, ? url/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac67b237dfc64d47a1ed4ddfdf9b81e1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dl Size...: 0 MiB [00:00, ? MiB/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "13f74510b4ce4e51ba9be21495469339"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "65636a4f02e245cfb00cb55112572fdd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train examples...: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "98d101cd87b641e49ecb3c5398759a3a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/incomplete.E406HJ_1.0.0/imdb_reviews-train.tfrecor…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "653d007237fd46379e6eca16778aef78"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test examples...: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "76ddf84428b446d094d2440901a0530b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/incomplete.E406HJ_1.0.0/imdb_reviews-test.tfrecord…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6fd1717c38d0419bb79d26fd82b33b64"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating unsupervised examples...: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c4e08ee398ad4251ac86510143ac9b20"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/incomplete.E406HJ_1.0.0/imdb_reviews-unsupervised.…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2b1130bbd3ff4abd895ea4ba469bb0c4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset imdb_reviews downloaded and prepared to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get subset of the data for training and testing (2000 samples each). Convert Keras dataset to lists of strings and labels."
      ],
      "metadata": {
        "id": "nn6foYqeOT-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = []\n",
        "y_train = []\n",
        "\n",
        "for sample, label in train_dataset.take(2000):\n",
        "  x_train.append(sample.numpy())\n",
        "  y_train.append(label.numpy())\n",
        "\n",
        "x_train = np.asarray(x_train)\n",
        "y_train = np.asarray(y_train)\n",
        "\n",
        "print(x_train[0])\n",
        "print(y_train[0])\n",
        "\n",
        "x_test = []\n",
        "y_test = []\n",
        "\n",
        "for sample, label in test_dataset.take(100):\n",
        "  x_test.append(sample.numpy())\n",
        "  y_test.append(label.numpy())\n",
        "\n",
        "x_test = np.asarray(x_test)\n",
        "y_test = np.asarray(y_test)\n",
        "\n",
        "print(x_test[0])\n",
        "print(y_test[0])"
      ],
      "metadata": {
        "id": "a1iCXfbCOR-w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db1e474c-271f-4fd0-f302-f3bab40ea1a4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"\n",
            "0\n",
            "b\"There are films that make careers. For George Romero, it was NIGHT OF THE LIVING DEAD; for Kevin Smith, CLERKS; for Robert Rodriguez, EL MARIACHI. Add to that list Onur Tukel's absolutely amazing DING-A-LING-LESS. Flawless film-making, and as assured and as professional as any of the aforementioned movies. I haven't laughed this hard since I saw THE FULL MONTY. (And, even then, I don't think I laughed quite this hard... So to speak.) Tukel's talent is considerable: DING-A-LING-LESS is so chock full of double entendres that one would have to sit down with a copy of this script and do a line-by-line examination of it to fully appreciate the, uh, breadth and width of it. Every shot is beautifully composed (a clear sign of a sure-handed director), and the performances all around are solid (there's none of the over-the-top scenery chewing one might've expected from a film like this). DING-A-LING-LESS is a film whose time has come.\"\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Add your comparisons (baseline + RNN)\n",
        "\n",
        "Here is the code for my comparison models from the deep learning part of the project."
      ],
      "metadata": {
        "id": "nN9FUZ6eOmB0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the experiments using Gemma and comparisons\n",
        "\n",
        "Here is the code I used to get the results below! Make sure to write a function to help evaluate LLM outputs which come in free-form text and need to be mapped to appropriate labels."
      ],
      "metadata": {
        "id": "xxR7SQXnOthj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TASK 1: LLM output parser to 0 (negative) or 1 (positive)\n"
      ],
      "metadata": {
        "id": "koJuA9wNPIKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def parse_llm(llm_output) -> int:\n",
        "    \"\"\"\n",
        "    Parses the text output from an LLM to extract a binary label, 0 (negative) or 1 (positive)\n",
        "\n",
        "    This function handles this types of outputs:\n",
        "    - Zero-shot\n",
        "    - Few-shot\n",
        "    - Chain-of-Thought (prioritize last occurrence of \"positive\" or negative)\n",
        "\n",
        "\n",
        "    Returns:\n",
        "        int:\n",
        "            1  if the sentiment is determined to be 'Positive'.\n",
        "            0  if the sentiment is determined to be 'Negative'.\n",
        "            -1 if the output is ambiguous or fits the 'catch-all' bucket\n",
        "               (neither label found).\n",
        "    \"\"\"\n",
        "\n",
        "    # Normalize the output: tolowercase and strip whitespace\n",
        "    text = str(llm_output).lower().strip()\n",
        "\n",
        "    label_positive = \"positive\"\n",
        "    label_negative = \"negative\"\n",
        "\n",
        "    # Find the LAST occurrence of each label.\n",
        "    # rfind() returns the highest index where the substring is found, or -1 if not found.\n",
        "    idx_pos = text.rfind(label_positive)\n",
        "    idx_neg = text.rfind(label_negative)\n",
        "\n",
        "    if idx_pos == -1 and idx_neg == -1:\n",
        "        # Catch-all bucket (Neither found)\n",
        "        return -1\n",
        "\n",
        "    if idx_pos > idx_neg:\n",
        "        # \"positive\" appears after \"negative\"\n",
        "        return 1\n",
        "    else:\n",
        "        # \"negative\" appears \"positive\"\n",
        "        return 0\n",
        "\n",
        "# ==========================================\n",
        "# Testing the Parser with Scenarios\n",
        "# ==========================================\n",
        "\n",
        "# Scenario 1: Clean Zero-shot output\n",
        "print(f\"Test 1 (Clean Positive): {parse_llm('Positive')}\")\n",
        "# Expected: 1\n",
        "\n",
        "# Scenario 2: Chain-of-Thought (Reasoning includes opposing word)\n",
        "cot_example = \"The movie had some positive moments, specifically the lighting, but overall the plot was boring. Therefore it is Negative.\"\n",
        "print(f\"Test 2 (CoT handling): {parse_llm(cot_example)}\")\n",
        "# Expected: 0 (Because 'Negative' appears last, overriding the earlier 'positive')\n",
        "\n",
        "# Scenario 3: Ambiguous / Garbage output\n",
        "garbage_example = \"I am not sure what you mean. The movie was okay.\"\n",
        "print(f\"Test 3 (Catch-all): {parse_llm(garbage_example)}\")\n",
        "# Expected: -1 (Neither label found)\n",
        "\n",
        "# Scenario 4: Case insensitivity and punctuation\n",
        "messy_example = \"   POSITIVE!!! \"\n",
        "print(f\"Test 4 (Normalization): {parse_llm(messy_example)}\")\n",
        "# Expected: 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5ouDAaKPF9i",
        "outputId": "255c05d4-4483-42a3-d08d-f45472e9a246"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 1 (Clean Positive): 1\n",
            "Test 2 (CoT handling): 0\n",
            "Test 3 (Catch-all): -1\n",
            "Test 4 (Normalization): 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TASK 2: Prompting"
      ],
      "metadata": {
        "id": "KlhpPnRAsTbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CPU llama-cpp-python\n",
        "!pip install llama-cpp-python\n",
        "\n",
        "# For easy download of the models\n",
        "!pip install huggingface_hub\n",
        "\n"
      ],
      "metadata": {
        "id": "xVINIrwx4zDg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37f6bc9c-fa57-4a07-e969-17045bb0aa9c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.3.16.tar.gz (50.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python) (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python) (2.0.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.3)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl size=4500173 sha256=1c80fa87cedbcf5ff0a21340af008157b7dc7b89b79d38d04f484d129b34542e\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/82/ab/8784ee3fb99ddb07fd36a679ddbe63122cc07718f6c1eb3be8\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.16\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.36.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Login into Hugging Face Hub\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token)"
      ],
      "metadata": {
        "id": "Q9ffSGO3wRNm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "instruct_model_name_or_path = \"google/gemma-3-4b-it-qat-q4_0-gguf\"\n",
        "instruct_model_basename = \"gemma-3-4b-it-q4_0.gguf\" # INSTRUCT MODEL\n",
        "\n",
        "instruct_model_path = hf_hub_download(repo_id=instruct_model_name_or_path, filename=instruct_model_basename)\n",
        "\n",
        "\n",
        "from llama_cpp import Llama\n",
        "\n",
        "llm = Llama(\n",
        "    model_path=instruct_model_path,\n",
        "    n_threads=2, # CPU cores\n",
        "    n_ctx=4096, # Context window size is increased from 512 to 4096 to fit the 4 examples in few_shot strategy\n",
        "    n_gpu_layers=-1\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d99d1b0314b34bdeb02bb7fbcb98c4c6",
            "e9c549beda564263bcaa87396538e84c",
            "1fabb5ea6489404983fdac0cb720b07b",
            "14a46c59df6e47b5b1069210a34c29c9",
            "5df52763770a4a848bbb86f3b56ba2ba",
            "a35383a72f5b437ab062a5868609d644",
            "7cf5e46684c2400eac91de7ec2770e95",
            "d7d08c8c40534b20b0e746493beca0ac",
            "bc80cf76fb6a43a4a5daa112070761cd",
            "7a3768e354e64fc698ee1c4b67649fe0",
            "d14746eee2934806bfca7f7e7564b238"
          ]
        },
        "id": "yG96j9NGvw-U",
        "outputId": "8d9560a1-2854-4d04-9aee-0af65bf5bd36"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "gemma-3-4b-it-q4_0.gguf:   0%|          | 0.00/3.16G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d99d1b0314b34bdeb02bb7fbcb98c4c6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 39 key-value pairs and 444 tensors from /root/.cache/huggingface/hub/models--google--gemma-3-4b-it-qat-q4_0-gguf/snapshots/15f73f5eee9c28f53afefef5723e29680c2fc78a/gemma-3-4b-it-q4_0.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = gemma3\n",
            "llama_model_loader: - kv   1:                      gemma3.context_length u32              = 131072\n",
            "llama_model_loader: - kv   2:                         gemma3.block_count u32              = 34\n",
            "llama_model_loader: - kv   3:                    gemma3.embedding_length u32              = 2560\n",
            "llama_model_loader: - kv   4:                 gemma3.feed_forward_length u32              = 10240\n",
            "llama_model_loader: - kv   5:                gemma3.attention.head_count u32              = 8\n",
            "llama_model_loader: - kv   6:             gemma3.attention.head_count_kv u32              = 4\n",
            "llama_model_loader: - kv   7:                gemma3.attention.key_length u32              = 256\n",
            "llama_model_loader: - kv   8:              gemma3.attention.value_length u32              = 256\n",
            "llama_model_loader: - kv   9:    gemma3.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  10:                   gemma3.rope.scaling.type str              = linear\n",
            "llama_model_loader: - kv  11:                 gemma3.rope.scaling.factor f32              = 8.000000\n",
            "llama_model_loader: - kv  12:                      gemma3.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  13:            gemma3.attention.sliding_window u32              = 1024\n",
            "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 2\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3\n",
            "llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,262144]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
            "llama_model_loader: - kv  20:                      tokenizer.ggml.scores arr[f32,262144]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,262144]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  23:                          general.file_type u32              = 2\n",
            "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{ bos_token }} {%- if messages[0]['r...\n",
            "llama_model_loader: - kv  25:                 gemma3.mm.tokens_per_image u32              = 256\n",
            "llama_model_loader: - kv  26:         gemma3.vision.attention.head_count u32              = 16\n",
            "llama_model_loader: - kv  27: gemma3.vision.attention.layer_norm_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  28:                  gemma3.vision.block_count u32              = 27\n",
            "llama_model_loader: - kv  29:             gemma3.vision.embedding_length u32              = 1152\n",
            "llama_model_loader: - kv  30:          gemma3.vision.feed_forward_length u32              = 4304\n",
            "llama_model_loader: - kv  31:                   gemma3.vision.image_size u32              = 896\n",
            "llama_model_loader: - kv  32:                 gemma3.vision.num_channels u32              = 3\n",
            "llama_model_loader: - kv  33:                   gemma3.vision.patch_size u32              = 14\n",
            "llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  36:           tokenizer.ggml.add_padding_token bool             = false\n",
            "llama_model_loader: - kv  37:           tokenizer.ggml.add_unknown_token bool             = false\n",
            "llama_model_loader: - kv  38:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - type  f32:  205 tensors\n",
            "llama_model_loader: - type  f16:    1 tensors\n",
            "llama_model_loader: - type q4_0:  238 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_0\n",
            "print_info: file size   = 2.93 GiB (6.49 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 1\n",
            "load: control token: 256000 '<end_of_image>' is not marked as EOG\n",
            "load: control token: 255999 '<start_of_image>' is not marked as EOG\n",
            "load: control token:    105 '<start_of_turn>' is not marked as EOG\n",
            "load: control token:      2 '<bos>' is not marked as EOG\n",
            "load: control token:      0 '<pad>' is not marked as EOG\n",
            "load: control token:      1 '<eos>' is not marked as EOG\n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "load: printing all EOG tokens:\n",
            "load:   - 1 ('<eos>')\n",
            "load:   - 106 ('<end_of_turn>')\n",
            "load: special tokens cache size = 8\n",
            "load: token to piece cache size = 1.9446 MB\n",
            "print_info: arch             = gemma3\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 131072\n",
            "print_info: n_embd           = 2560\n",
            "print_info: n_layer          = 34\n",
            "print_info: n_head           = 8\n",
            "print_info: n_head_kv        = 4\n",
            "print_info: n_rot            = 256\n",
            "print_info: n_swa            = 1024\n",
            "print_info: is_swa_any       = 1\n",
            "print_info: n_embd_head_k    = 256\n",
            "print_info: n_embd_head_v    = 256\n",
            "print_info: n_gqa            = 2\n",
            "print_info: n_embd_k_gqa     = 1024\n",
            "print_info: n_embd_v_gqa     = 1024\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-06\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 6.2e-02\n",
            "print_info: n_ff             = 10240\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 1000000.0\n",
            "print_info: freq_scale_train = 0.125\n",
            "print_info: n_ctx_orig_yarn  = 131072\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 4B\n",
            "print_info: model params     = 3.88 B\n",
            "print_info: general.name     = n/a\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 262144\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 2 '<bos>'\n",
            "print_info: EOS token        = 1 '<eos>'\n",
            "print_info: EOT token        = 106 '<end_of_turn>'\n",
            "print_info: UNK token        = 3 '<unk>'\n",
            "print_info: PAD token        = 0 '<pad>'\n",
            "print_info: LF token         = 248 '<0x0A>'\n",
            "print_info: EOG token        = 1 '<eos>'\n",
            "print_info: EOG token        = 106 '<end_of_turn>'\n",
            "print_info: max token length = 93\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CPU, is_swa = 1\n",
            "load_tensors: layer   1 assigned to device CPU, is_swa = 1\n",
            "load_tensors: layer   2 assigned to device CPU, is_swa = 1\n",
            "load_tensors: layer   3 assigned to device CPU, is_swa = 1\n",
            "load_tensors: layer   4 assigned to device CPU, is_swa = 1\n",
            "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   6 assigned to device CPU, is_swa = 1\n",
            "load_tensors: layer   7 assigned to device CPU, is_swa = 1\n",
            "load_tensors: layer   8 assigned to device CPU, is_swa = 1\n",
            "load_tensors: layer   9 assigned to device CPU, is_swa = 1\n",
            "load_tensors: layer  10 assigned to device CPU, is_swa = 1\n",
            "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  12 assigned to device CPU, is_swa = 1\n",
            "load_tensors: layer  13 assigned to device CPU, is_swa = 1\n",
            "load_tensors: layer  14 assigned to device CPU, is_swa = 1\n",
            "load_tensors: layer  15 assigned to device CPU, is_swa = 1\n",
            "load_tensors: layer  16 assigned to device CPU, is_swa = 1\n",
            "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  18 assigned to device CPU, is_swa = 1\n",
            "load_tensors: layer  19 assigned to device CPU, is_swa = 1\n",
            "load_tensors: layer  20 assigned to device CPU, is_swa = 1\n",
            "load_tensors: layer  21 assigned to device CPU, is_swa = 1\n",
            "load_tensors: layer  22 assigned to device CPU, is_swa = 1\n",
            "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  24 assigned to device CPU, is_swa = 1\n",
            "load_tensors: layer  25 assigned to device CPU, is_swa = 1\n",
            "load_tensors: layer  26 assigned to device CPU, is_swa = 1\n",
            "load_tensors: layer  27 assigned to device CPU, is_swa = 1\n",
            "load_tensors: layer  28 assigned to device CPU, is_swa = 1\n",
            "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  30 assigned to device CPU, is_swa = 1\n",
            "load_tensors: layer  31 assigned to device CPU, is_swa = 1\n",
            "load_tensors: layer  32 assigned to device CPU, is_swa = 1\n",
            "load_tensors: layer  33 assigned to device CPU, is_swa = 1\n",
            "load_tensors: layer  34 assigned to device CPU, is_swa = 0\n",
            "load_tensors: tensor 'token_embd.weight' (f16) (and 206 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
            "load_tensors:   CPU_REPACK model buffer size =  1721.25 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size =  3002.65 MiB\n",
            "repack: repack tensor blk.0.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.0.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.0.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.0.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.0.ffn_gate.weight with q4_0_8x8\n",
            "repack: repack tensor blk.0.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.0.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.1.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.1.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.1.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.1.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.1.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.1.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.1.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.2.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.2.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.2.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.2.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.2.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.2.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.2.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.3.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.3.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.3.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.3.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.3.ffn_gate.weight with q4_0_8x8\n",
            "repack: repack tensor blk.3.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.3.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.4.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.4.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.4.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.4.attn_output.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.4.ffn_gate.weight with q4_0_8x8\n",
            "repack: repack tensor blk.4.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.4.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.5.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.5.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.5.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.5.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.5.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.5.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.5.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.6.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.6.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.6.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.6.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.6.ffn_gate.weight with q4_0_8x8\n",
            "repack: repack tensor blk.6.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.6.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.7.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.7.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.7.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.7.attn_output.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.7.ffn_gate.weight with q4_0_8x8\n",
            "repack: repack tensor blk.7.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.7.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.8.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.8.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.8.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.8.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.8.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.8.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.8.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.9.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.9.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.9.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.9.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.9.ffn_gate.weight with q4_0_8x8\n",
            "repack: repack tensor blk.9.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.9.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.10.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.10.attn_k.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.10.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.10.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.10.ffn_gate.weight with q4_0_8x8\n",
            "repack: repack tensor blk.10.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.10.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.11.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.11.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.11.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.11.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.11.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.11.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.11.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.12.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.12.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.12.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.12.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.12.ffn_gate.weight with q4_0_8x8\n",
            "repack: repack tensor blk.12.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.12.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.13.attn_q.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.13.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.13.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.13.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.13.ffn_gate.weight with q4_0_8x8\n",
            "repack: repack tensor blk.13.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.13.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.14.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.14.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.14.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.14.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.14.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.14.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.14.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.15.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.15.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.15.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.15.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.15.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.15.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.15.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.16.attn_q.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.16.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.16.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.16.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.16.ffn_gate.weight with q4_0_8x8\n",
            "repack: repack tensor blk.16.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.16.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.17.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.17.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.17.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.17.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.17.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.17.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.17.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.18.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.18.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.18.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.18.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.18.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.18.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.18.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.19.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.19.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.19.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.19.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.19.ffn_gate.weight with q4_0_8x8\n",
            "repack: repack tensor blk.19.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.19.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.20.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.20.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.20.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.20.attn_output.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.20.ffn_gate.weight with q4_0_8x8\n",
            "repack: repack tensor blk.20.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.20.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.21.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.21.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.21.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.21.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.21.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.21.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.21.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.22.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.22.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.22.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.22.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.22.ffn_gate.weight with q4_0_8x8\n",
            "repack: repack tensor blk.22.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.22.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.23.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.23.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.23.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.23.attn_output.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.23.ffn_gate.weight with q4_0_8x8\n",
            "repack: repack tensor blk.23.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.23.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.24.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.24.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.24.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.24.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.24.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.24.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.24.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.25.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.25.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.25.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.25.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.25.ffn_gate.weight with q4_0_8x8\n",
            "repack: repack tensor blk.25.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.25.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.26.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.26.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.26.attn_v.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.26.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.26.ffn_gate.weight with q4_0_8x8\n",
            "repack: repack tensor blk.26.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.26.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.27.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.27.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.27.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.27.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.27.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.27.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.27.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.28.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.28.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.28.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.28.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.28.ffn_gate.weight with q4_0_8x8\n",
            "repack: repack tensor blk.28.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.28.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.29.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.29.attn_k.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.29.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.29.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.29.ffn_gate.weight with q4_0_8x8\n",
            "repack: repack tensor blk.29.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.29.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.30.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.30.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.30.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.30.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.30.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.30.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.30.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.31.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.31.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.31.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.31.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.31.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.31.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.31.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.32.attn_q.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.32.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.32.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.32.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.32.ffn_gate.weight with q4_0_8x8\n",
            "repack: repack tensor blk.32.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.32.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.33.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.33.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.33.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.33.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.33.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.33.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.33.ffn_down.weight with q4_0_8x8\n",
            "...\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 4096\n",
            "llama_context: n_ctx_per_seq = 4096\n",
            "llama_context: n_batch       = 512\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: kv_unified    = false\n",
            "llama_context: freq_base     = 1000000.0\n",
            "llama_context: freq_scale    = 0.125\n",
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "set_abort_callback: call\n",
            "llama_context:        CPU  output buffer size =     1.00 MiB\n",
            "create_memory: n_ctx = 4096 (padded)\n",
            "llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\n",
            "llama_kv_cache_unified_iswa: creating non-SWA KV cache, size = 4096 cells\n",
            "llama_kv_cache_unified: layer   0: skipped\n",
            "llama_kv_cache_unified: layer   1: skipped\n",
            "llama_kv_cache_unified: layer   2: skipped\n",
            "llama_kv_cache_unified: layer   3: skipped\n",
            "llama_kv_cache_unified: layer   4: skipped\n",
            "llama_kv_cache_unified: layer   5: dev = CPU\n",
            "llama_kv_cache_unified: layer   6: skipped\n",
            "llama_kv_cache_unified: layer   7: skipped\n",
            "llama_kv_cache_unified: layer   8: skipped\n",
            "llama_kv_cache_unified: layer   9: skipped\n",
            "llama_kv_cache_unified: layer  10: skipped\n",
            "llama_kv_cache_unified: layer  11: dev = CPU\n",
            "llama_kv_cache_unified: layer  12: skipped\n",
            "llama_kv_cache_unified: layer  13: skipped\n",
            "llama_kv_cache_unified: layer  14: skipped\n",
            "llama_kv_cache_unified: layer  15: skipped\n",
            "llama_kv_cache_unified: layer  16: skipped\n",
            "llama_kv_cache_unified: layer  17: dev = CPU\n",
            "llama_kv_cache_unified: layer  18: skipped\n",
            "llama_kv_cache_unified: layer  19: skipped\n",
            "llama_kv_cache_unified: layer  20: skipped\n",
            "llama_kv_cache_unified: layer  21: skipped\n",
            "llama_kv_cache_unified: layer  22: skipped\n",
            "llama_kv_cache_unified: layer  23: dev = CPU\n",
            "llama_kv_cache_unified: layer  24: skipped\n",
            "llama_kv_cache_unified: layer  25: skipped\n",
            "llama_kv_cache_unified: layer  26: skipped\n",
            "llama_kv_cache_unified: layer  27: skipped\n",
            "llama_kv_cache_unified: layer  28: skipped\n",
            "llama_kv_cache_unified: layer  29: dev = CPU\n",
            "llama_kv_cache_unified: layer  30: skipped\n",
            "llama_kv_cache_unified: layer  31: skipped\n",
            "llama_kv_cache_unified: layer  32: skipped\n",
            "llama_kv_cache_unified: layer  33: skipped\n",
            "llama_kv_cache_unified:        CPU KV buffer size =    80.00 MiB\n",
            "llama_kv_cache_unified: size =   80.00 MiB (  4096 cells,   5 layers,  1/1 seqs), K (f16):   40.00 MiB, V (f16):   40.00 MiB\n",
            "llama_kv_cache_unified_iswa: creating     SWA KV cache, size = 4096 cells\n",
            "llama_kv_cache_unified: layer   0: dev = CPU\n",
            "llama_kv_cache_unified: layer   1: dev = CPU\n",
            "llama_kv_cache_unified: layer   2: dev = CPU\n",
            "llama_kv_cache_unified: layer   3: dev = CPU\n",
            "llama_kv_cache_unified: layer   4: dev = CPU\n",
            "llama_kv_cache_unified: layer   5: skipped\n",
            "llama_kv_cache_unified: layer   6: dev = CPU\n",
            "llama_kv_cache_unified: layer   7: dev = CPU\n",
            "llama_kv_cache_unified: layer   8: dev = CPU\n",
            "llama_kv_cache_unified: layer   9: dev = CPU\n",
            "llama_kv_cache_unified: layer  10: dev = CPU\n",
            "llama_kv_cache_unified: layer  11: skipped\n",
            "llama_kv_cache_unified: layer  12: dev = CPU\n",
            "llama_kv_cache_unified: layer  13: dev = CPU\n",
            "llama_kv_cache_unified: layer  14: dev = CPU\n",
            "llama_kv_cache_unified: layer  15: dev = CPU\n",
            "llama_kv_cache_unified: layer  16: dev = CPU\n",
            "llama_kv_cache_unified: layer  17: skipped\n",
            "llama_kv_cache_unified: layer  18: dev = CPU\n",
            "llama_kv_cache_unified: layer  19: dev = CPU\n",
            "llama_kv_cache_unified: layer  20: dev = CPU\n",
            "llama_kv_cache_unified: layer  21: dev = CPU\n",
            "llama_kv_cache_unified: layer  22: dev = CPU\n",
            "llama_kv_cache_unified: layer  23: skipped\n",
            "llama_kv_cache_unified: layer  24: dev = CPU\n",
            "llama_kv_cache_unified: layer  25: dev = CPU\n",
            "llama_kv_cache_unified: layer  26: dev = CPU\n",
            "llama_kv_cache_unified: layer  27: dev = CPU\n",
            "llama_kv_cache_unified: layer  28: dev = CPU\n",
            "llama_kv_cache_unified: layer  29: skipped\n",
            "llama_kv_cache_unified: layer  30: dev = CPU\n",
            "llama_kv_cache_unified: layer  31: dev = CPU\n",
            "llama_kv_cache_unified: layer  32: dev = CPU\n",
            "llama_kv_cache_unified: layer  33: dev = CPU\n",
            "llama_kv_cache_unified:        CPU KV buffer size =   464.00 MiB\n",
            "llama_kv_cache_unified: size =  464.00 MiB (  4096 cells,  29 layers,  1/1 seqs), K (f16):  232.00 MiB, V (f16):  232.00 MiB\n",
            "llama_context: enumerating backends\n",
            "llama_context: backend_ptrs.size() = 1\n",
            "llama_context: max_nodes = 3552\n",
            "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "llama_context:        CPU compute buffer size =   517.00 MiB\n",
            "llama_context: graph nodes  = 1503\n",
            "llama_context: graph splits = 1\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | AVX512_VNNI = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "Model metadata: {'tokenizer.ggml.add_unknown_token': 'false', 'tokenizer.ggml.add_padding_token': 'false', 'tokenizer.ggml.add_eos_token': 'false', 'gemma3.vision.num_channels': '3', 'gemma3.rope.freq_base': '1000000.000000', 'gemma3.rope.scaling.type': 'linear', 'gemma3.attention.layer_norm_rms_epsilon': '0.000001', 'gemma3.vision.attention.layer_norm_epsilon': '0.000001', 'gemma3.attention.key_length': '256', 'gemma3.attention.head_count_kv': '4', 'gemma3.attention.head_count': '8', 'tokenizer.chat_template': '{{ bos_token }} {%- if messages[0][\\'role\\'] == \\'system\\' -%} {%- if messages[0][\\'content\\'] is string -%} {%- set first_user_prefix = messages[0][\\'content\\'] + \\'\\\\n\\' -%} {%- else -%} {%- set first_user_prefix = messages[0][\\'content\\'][0][\\'text\\'] + \\'\\\\n\\' -%} {%- endif -%} {%- set loop_messages = messages[1:] -%} {%- else -%} {%- set first_user_prefix = \"\" -%} {%- set loop_messages = messages -%} {%- endif -%} {%- for message in loop_messages -%} {%- if (message[\\'role\\'] == \\'user\\') != (loop.index0 % 2 == 0) -%} {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }} {%- endif -%} {%- if (message[\\'role\\'] == \\'assistant\\') -%} {%- set role = \"model\" -%} {%- else -%} {%- set role = message[\\'role\\'] -%} {%- endif -%} {{ \\'<start_of_turn>\\' + role + \\'\\\\n\\' + (first_user_prefix if loop.first else \"\") }} {%- if message[\\'content\\'] is string -%} {{ message[\\'content\\'] | trim }} {%- elif message[\\'content\\'] is iterable -%} {%- for item in message[\\'content\\'] -%} {%- if item[\\'type\\'] == \\'image\\' -%} {{ \\'<start_of_image>\\' }} {%- elif item[\\'type\\'] == \\'text\\' -%} {{ item[\\'text\\'] | trim }} {%- endif -%} {%- endfor -%} {%- else -%} {{ raise_exception(\"Invalid content type\") }} {%- endif -%} {{ \\'<end_of_turn>\\\\n\\' }} {%- endfor -%} {%- if add_generation_prompt -%} {{\\'<start_of_turn>model\\\\n\\'}} {%- endif -%}', 'gemma3.attention.value_length': '256', 'gemma3.context_length': '131072', 'tokenizer.ggml.add_bos_token': 'true', 'gemma3.feed_forward_length': '10240', 'general.architecture': 'gemma3', 'gemma3.vision.block_count': '27', 'gemma3.attention.sliding_window': '1024', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'gemma3.vision.patch_size': '14', 'tokenizer.ggml.bos_token_id': '2', 'gemma3.block_count': '34', 'gemma3.vision.attention.head_count': '16', 'gemma3.rope.scaling.factor': '8.000000', 'gemma3.vision.feed_forward_length': '4304', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'gemma3.embedding_length': '2560', 'general.file_type': '2', 'gemma3.mm.tokens_per_image': '256', 'tokenizer.ggml.pre': 'default', 'gemma3.vision.embedding_length': '1152', 'gemma3.vision.image_size': '896'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {{ bos_token }} {%- if messages[0]['role'] == 'system' -%} {%- if messages[0]['content'] is string -%} {%- set first_user_prefix = messages[0]['content'] + '\\n' -%} {%- else -%} {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\\n' -%} {%- endif -%} {%- set loop_messages = messages[1:] -%} {%- else -%} {%- set first_user_prefix = \"\" -%} {%- set loop_messages = messages -%} {%- endif -%} {%- for message in loop_messages -%} {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%} {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }} {%- endif -%} {%- if (message['role'] == 'assistant') -%} {%- set role = \"model\" -%} {%- else -%} {%- set role = message['role'] -%} {%- endif -%} {{ '<start_of_turn>' + role + '\\n' + (first_user_prefix if loop.first else \"\") }} {%- if message['content'] is string -%} {{ message['content'] | trim }} {%- elif message['content'] is iterable -%} {%- for item in message['content'] -%} {%- if item['type'] == 'image' -%} {{ '<start_of_image>' }} {%- elif item['type'] == 'text' -%} {{ item['text'] | trim }} {%- endif -%} {%- endfor -%} {%- else -%} {{ raise_exception(\"Invalid content type\") }} {%- endif -%} {{ '<end_of_turn>\\n' }} {%- endfor -%} {%- if add_generation_prompt -%} {{'<start_of_turn>model\\n'}} {%- endif -%}\n",
            "Using chat eos_token: <eos>\n",
            "Using chat bos_token: <bos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Notice that we have no layers on the GPU\n",
        "print(\"GPU Layers\")\n",
        "print(llm.model_params.n_gpu_layers)\n",
        "print(\"CPU Layers\")\n",
        "from llama_cpp import llama_n_layer\n",
        "print(llama_n_layer(llm.model))\n"
      ],
      "metadata": {
        "id": "VCjEVmhtv4xU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dd939fc-5aab-4689-a225-883a69d63867"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Layers\n",
            "2147483647\n",
            "CPU Layers\n",
            "34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_examples_data = []\n",
        "count_positive = 0\n",
        "count_negative = 0\n",
        "for i in range(10):\n",
        "    if (count_positive < 2 and y_train[i] == 1):\n",
        "      few_shot_examples_data.append((x_train[i], y_train[i]))\n",
        "      count_positive += 1\n",
        "    if (count_negative < 2 and y_train[i] == 0):\n",
        "      few_shot_examples_data.append((x_train[i], y_train[i]))\n",
        "      count_negative += 1\n",
        "for i in range(4):\n",
        "  print(few_shot_examples_data[i])\n",
        "\n",
        "def get_chat_messages(strategy, review_text):\n",
        "\n",
        "    # Check if the text is currently in bytes format ( b\"This is bytes format\")\n",
        "    if isinstance(review_text, bytes):\n",
        "        # If true, decode it into a standard string (\"Hello world\")\n",
        "        review_text = review_text.decode('utf-8')\n",
        "    else:\n",
        "        # If false (it's already a string), keep it exactly as it is\n",
        "        review_text = review_text\n",
        "\n",
        "    base_instruction = (\n",
        "        \"You are a movie review sentiment classifier. \"\n",
        "        \"Classify the following review as either 'positive' or 'negative'. \"\n",
        "        \"Do not output anything else.\"\n",
        "    )\n",
        "\n",
        "    messages = []\n",
        "\n",
        "    if strategy == \"zero_shot\":\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": f\"{base_instruction}\\n\\nReview: {review_text}\\nSentiment:\"}\n",
        "        ]\n",
        "\n",
        "    elif strategy == \"cot\":\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": (\n",
        "                f\"{base_instruction}\\n\\n\"\n",
        "                f\"Review: {review_text}\\n\"\n",
        "                \"Let's think step by step to determine the sentiment. \"\n",
        "                \"Finally, conclude with 'Therefore, the sentiment is [positive/negative]'.\"\n",
        "            )}\n",
        "        ]\n",
        "\n",
        "    elif strategy == \"few_shot\":\n",
        "\n",
        "        messages.append({\"role\": \"system\", \"content\": base_instruction})\n",
        "\n",
        "        # Add the 4 examples as a conversation history\n",
        "\n",
        "        for ex_text, ex_label in few_shot_examples_data:\n",
        "            # Decode the example text if it is bytes\n",
        "            if isinstance(ex_text, bytes):\n",
        "                ex_text = ex_text.decode('utf-8')\n",
        "\n",
        "            label_str = \"positive\" if ex_label == 1 else \"negative\"\n",
        "            messages.append({\"role\": \"user\", \"content\": f\"Review: {ex_text}\\nSentiment:\"})\n",
        "            messages.append({\"role\": \"assistant\", \"content\": label_str})\n",
        "\n",
        "        # Finally, add the target review\n",
        "        messages.append({\"role\": \"user\", \"content\": f\"Review: {review_text}\\nSentiment:\"})\n",
        "\n",
        "    return messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsiBWzBa9O1B",
        "outputId": "714b2cd0-081b-4edf-daa9-34919c7886ac"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(np.bytes_(b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"), np.int64(0))\n",
            "(np.bytes_(b'I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.'), np.int64(0))\n",
            "(np.bytes_(b'This is the kind of film for a snowy Sunday afternoon when the rest of the world can go ahead with its own business as you descend into a big arm-chair and mellow for a couple of hours. Wonderful performances from Cher and Nicolas Cage (as always) gently row the plot along. There are no rapids to cross, no dangerous waters, just a warm and witty paddle through New York life at its best. A family film in every sense and one that deserves the praise it received.'), np.int64(1))\n",
            "(np.bytes_(b'As others have mentioned, all the women that go nude in this film are mostly absolutely gorgeous. The plot very ably shows the hypocrisy of the female libido. When men are around they want to be pursued, but when no \"men\" are around, they become the pursuers of a 14 year old boy. And the boy becomes a man really fast (we should all be so lucky at this age!). He then gets up the courage to pursue his true love.'), np.int64(1))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import sklearn.metrics\n",
        "\n",
        "def evaluate_model_llama(strategy, x_data, y_true, limit=100):\n",
        "    predictions = []\n",
        "    ground_truth = []\n",
        "\n",
        "    print(f\"Running evaluation for strategy: {strategy}...\")\n",
        "\n",
        "    for i in tqdm(range(limit)):\n",
        "        review = x_data[i]\n",
        "        label = y_true[i]\n",
        "\n",
        "        # 1. Get messages based on strategy\n",
        "        messages = get_chat_messages(strategy, review)\n",
        "\n",
        "        # 2. Generate Output using Llama.cpp Chat API\n",
        "        try:\n",
        "            response = llm.create_chat_completion(\n",
        "                messages=messages,\n",
        "                max_tokens=100,      # Give enough room for CoT\n",
        "                temperature=0.1,     # Lower temperature for deterministic classification\n",
        "                top_p=0.95\n",
        "            )\n",
        "\n",
        "            # Extract text content\n",
        "            generated_text = response['choices'][0]['message']['content']\n",
        "\n",
        "            # 3. Parse Sentiment (using the function from Task 1)\n",
        "            pred_label = parse_llm(generated_text)\n",
        "\n",
        "            # 4. Handle invalid outputs\n",
        "            if pred_label == -1:\n",
        "                pred_label = 1 - label # Force error if parser failed\n",
        "\n",
        "            predictions.append(pred_label)\n",
        "            ground_truth.append(label)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing sample {i}: {e}\")\n",
        "            predictions.append(1 - label) # Count error as failure\n",
        "            ground_truth.append(label)\n",
        "\n",
        "    # Calculate metrics\n",
        "    acc = sklearn.metrics.accuracy_score(ground_truth, predictions)\n",
        "    cm = sklearn.metrics.confusion_matrix(ground_truth, predictions)\n",
        "\n",
        "    print(f\"Accuracy for {strategy}: {acc:.2f}\")\n",
        "    return acc, cm\n",
        "\n",
        "# Run the experiments\n",
        "acc_zero, cm_zero = evaluate_model_llama(\"zero_shot\", x_test, y_test)\n",
        "acc_few, cm_few = evaluate_model_llama(\"few_shot\", x_test, y_test)\n",
        "acc_cot, cm_cot = evaluate_model_llama(\"cot\", x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVsP1VBo9RtU",
        "outputId": "cf78b560-648c-4a03-a097-9e687382a7ca"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running evaluation for strategy: zero_shot...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/100 [00:00<?, ?it/s]llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3933.85 ms /   281 tokens (   14.00 ms per token,    71.43 tokens per second)\n",
            "llama_perf_context_print:        eval time =     404.28 ms /     2 runs   (  202.14 ms per token,     4.95 tokens per second)\n",
            "llama_perf_context_print:       total time =    4347.70 ms /   283 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            "  1%|          | 1/100 [00:04<07:10,  4.35s/it]Llama.generate: 35 prefix-match hit, remaining 385 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    5781.18 ms /   385 tokens (   15.02 ms per token,    66.60 tokens per second)\n",
            "llama_perf_context_print:        eval time =     399.66 ms /     2 runs   (  199.83 ms per token,     5.00 tokens per second)\n",
            "llama_perf_context_print:       total time =    6189.77 ms /   387 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            "  2%|▏         | 2/100 [00:10<08:52,  5.44s/it]Llama.generate: 35 prefix-match hit, remaining 679 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =   10706.11 ms /   679 tokens (   15.77 ms per token,    63.42 tokens per second)\n",
            "llama_perf_context_print:        eval time =     412.17 ms /     2 runs   (  206.09 ms per token,     4.85 tokens per second)\n",
            "llama_perf_context_print:       total time =   11127.51 ms /   681 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            "  3%|▎         | 3/100 [00:21<12:59,  8.04s/it]Llama.generate: 35 prefix-match hit, remaining 352 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    4684.15 ms /   352 tokens (   13.31 ms per token,    75.15 tokens per second)\n",
            "llama_perf_context_print:        eval time =     404.91 ms /     2 runs   (  202.45 ms per token,     4.94 tokens per second)\n",
            "llama_perf_context_print:       total time =    5097.59 ms /   354 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            "  4%|▍         | 4/100 [00:26<11:00,  6.88s/it]Llama.generate: 35 prefix-match hit, remaining 103 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    1460.45 ms /   103 tokens (   14.18 ms per token,    70.53 tokens per second)\n",
            "llama_perf_context_print:        eval time =     397.94 ms /     2 runs   (  198.97 ms per token,     5.03 tokens per second)\n",
            "llama_perf_context_print:       total time =    1866.41 ms /   105 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            "  5%|▌         | 5/100 [00:28<08:02,  5.07s/it]Llama.generate: 35 prefix-match hit, remaining 187 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3967.24 ms /   187 tokens (   21.22 ms per token,    47.14 tokens per second)\n",
            "llama_perf_context_print:        eval time =     397.72 ms /     2 runs   (  198.86 ms per token,     5.03 tokens per second)\n",
            "llama_perf_context_print:       total time =    4373.24 ms /   189 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            "  6%|▌         | 6/100 [00:33<07:34,  4.84s/it]Llama.generate: 35 prefix-match hit, remaining 129 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    1768.05 ms /   129 tokens (   13.71 ms per token,    72.96 tokens per second)\n",
            "llama_perf_context_print:        eval time =     390.43 ms /     2 runs   (  195.21 ms per token,     5.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    2166.63 ms /   131 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            "  7%|▋         | 7/100 [00:35<06:08,  3.97s/it]Llama.generate: 35 prefix-match hit, remaining 166 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2249.79 ms /   166 tokens (   13.55 ms per token,    73.78 tokens per second)\n",
            "llama_perf_context_print:        eval time =     401.46 ms /     2 runs   (  200.73 ms per token,     4.98 tokens per second)\n",
            "llama_perf_context_print:       total time =    2659.38 ms /   168 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            "  8%|▊         | 8/100 [00:37<05:26,  3.55s/it]Llama.generate: 35 prefix-match hit, remaining 291 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    4007.99 ms /   291 tokens (   13.77 ms per token,    72.61 tokens per second)\n",
            "llama_perf_context_print:        eval time =     402.84 ms /     2 runs   (  201.42 ms per token,     4.96 tokens per second)\n",
            "llama_perf_context_print:       total time =    4419.35 ms /   293 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            "  9%|▉         | 9/100 [00:42<05:48,  3.82s/it]Llama.generate: 35 prefix-match hit, remaining 536 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    8323.90 ms /   536 tokens (   15.53 ms per token,    64.39 tokens per second)\n",
            "llama_perf_context_print:        eval time =     400.04 ms /     2 runs   (  200.02 ms per token,     5.00 tokens per second)\n",
            "llama_perf_context_print:       total time =    8732.56 ms /   538 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 10%|█         | 10/100 [00:51<08:00,  5.34s/it]Llama.generate: 35 prefix-match hit, remaining 627 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    9663.11 ms /   627 tokens (   15.41 ms per token,    64.89 tokens per second)\n",
            "llama_perf_context_print:        eval time =     408.07 ms /     2 runs   (  204.04 ms per token,     4.90 tokens per second)\n",
            "llama_perf_context_print:       total time =   10079.90 ms /   629 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 11%|█         | 11/100 [01:01<10:04,  6.79s/it]Llama.generate: 36 prefix-match hit, remaining 164 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2199.76 ms /   164 tokens (   13.41 ms per token,    74.55 tokens per second)\n",
            "llama_perf_context_print:        eval time =     397.12 ms /     2 runs   (  198.56 ms per token,     5.04 tokens per second)\n",
            "llama_perf_context_print:       total time =    2605.21 ms /   166 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 12%|█▏        | 12/100 [01:03<08:05,  5.52s/it]Llama.generate: 35 prefix-match hit, remaining 247 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3575.00 ms /   247 tokens (   14.47 ms per token,    69.09 tokens per second)\n",
            "llama_perf_context_print:        eval time =     401.72 ms /     2 runs   (  200.86 ms per token,     4.98 tokens per second)\n",
            "llama_perf_context_print:       total time =    3985.11 ms /   249 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 13%|█▎        | 13/100 [01:07<07:20,  5.06s/it]Llama.generate: 35 prefix-match hit, remaining 247 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    4037.31 ms /   247 tokens (   16.35 ms per token,    61.18 tokens per second)\n",
            "llama_perf_context_print:        eval time =     409.30 ms /     2 runs   (  204.65 ms per token,     4.89 tokens per second)\n",
            "llama_perf_context_print:       total time =    4454.91 ms /   249 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 14%|█▍        | 14/100 [01:12<06:59,  4.88s/it]Llama.generate: 35 prefix-match hit, remaining 274 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3667.52 ms /   274 tokens (   13.39 ms per token,    74.71 tokens per second)\n",
            "llama_perf_context_print:        eval time =     394.66 ms /     2 runs   (  197.33 ms per token,     5.07 tokens per second)\n",
            "llama_perf_context_print:       total time =    4070.49 ms /   276 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 15%|█▌        | 15/100 [01:16<06:34,  4.64s/it]Llama.generate: 35 prefix-match hit, remaining 167 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2268.56 ms /   167 tokens (   13.58 ms per token,    73.61 tokens per second)\n",
            "llama_perf_context_print:        eval time =     406.17 ms /     2 runs   (  203.08 ms per token,     4.92 tokens per second)\n",
            "llama_perf_context_print:       total time =    2683.07 ms /   169 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 16%|█▌        | 16/100 [01:18<05:40,  4.05s/it]Llama.generate: 35 prefix-match hit, remaining 265 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    4816.35 ms /   265 tokens (   18.17 ms per token,    55.02 tokens per second)\n",
            "llama_perf_context_print:        eval time =     396.89 ms /     2 runs   (  198.44 ms per token,     5.04 tokens per second)\n",
            "llama_perf_context_print:       total time =    5221.53 ms /   267 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 17%|█▋        | 17/100 [01:24<06:05,  4.40s/it]Llama.generate: 35 prefix-match hit, remaining 263 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3529.21 ms /   263 tokens (   13.42 ms per token,    74.52 tokens per second)\n",
            "llama_perf_context_print:        eval time =     396.57 ms /     2 runs   (  198.29 ms per token,     5.04 tokens per second)\n",
            "llama_perf_context_print:       total time =    3934.18 ms /   265 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 18%|█▊        | 18/100 [01:28<05:49,  4.26s/it]Llama.generate: 35 prefix-match hit, remaining 187 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2535.29 ms /   187 tokens (   13.56 ms per token,    73.76 tokens per second)\n",
            "llama_perf_context_print:        eval time =     397.20 ms /     2 runs   (  198.60 ms per token,     5.04 tokens per second)\n",
            "llama_perf_context_print:       total time =    2940.68 ms /   189 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 19%|█▉        | 19/100 [01:31<05:13,  3.87s/it]Llama.generate: 35 prefix-match hit, remaining 209 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    4110.36 ms /   209 tokens (   19.67 ms per token,    50.85 tokens per second)\n",
            "llama_perf_context_print:        eval time =     392.00 ms /     2 runs   (  196.00 ms per token,     5.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    4510.55 ms /   211 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 20%|██        | 20/100 [01:35<05:24,  4.06s/it]Llama.generate: 35 prefix-match hit, remaining 87 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    1225.61 ms /    87 tokens (   14.09 ms per token,    70.99 tokens per second)\n",
            "llama_perf_context_print:        eval time =     393.76 ms /     2 runs   (  196.88 ms per token,     5.08 tokens per second)\n",
            "llama_perf_context_print:       total time =    1627.45 ms /    89 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 21%|██        | 21/100 [01:37<04:23,  3.33s/it]Llama.generate: 35 prefix-match hit, remaining 370 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    4899.38 ms /   370 tokens (   13.24 ms per token,    75.52 tokens per second)\n",
            "llama_perf_context_print:        eval time =     402.36 ms /     2 runs   (  201.18 ms per token,     4.97 tokens per second)\n",
            "llama_perf_context_print:       total time =    5310.32 ms /   372 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 22%|██▏       | 22/100 [01:42<05:06,  3.93s/it]Llama.generate: 35 prefix-match hit, remaining 189 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3399.98 ms /   189 tokens (   17.99 ms per token,    55.59 tokens per second)\n",
            "llama_perf_context_print:        eval time =     395.59 ms /     2 runs   (  197.80 ms per token,     5.06 tokens per second)\n",
            "llama_perf_context_print:       total time =    3803.86 ms /   191 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 23%|██▎       | 23/100 [01:46<04:59,  3.89s/it]Llama.generate: 35 prefix-match hit, remaining 145 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2220.07 ms /   145 tokens (   15.31 ms per token,    65.31 tokens per second)\n",
            "llama_perf_context_print:        eval time =     390.34 ms /     2 runs   (  195.17 ms per token,     5.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    2618.49 ms /   147 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 24%|██▍       | 24/100 [01:48<04:26,  3.51s/it]Llama.generate: 35 prefix-match hit, remaining 256 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3387.57 ms /   256 tokens (   13.23 ms per token,    75.57 tokens per second)\n",
            "llama_perf_context_print:        eval time =     392.14 ms /     2 runs   (  196.07 ms per token,     5.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    3787.99 ms /   258 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 25%|██▌       | 25/100 [01:52<04:29,  3.60s/it]Llama.generate: 35 prefix-match hit, remaining 164 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2205.29 ms /   164 tokens (   13.45 ms per token,    74.37 tokens per second)\n",
            "llama_perf_context_print:        eval time =     395.33 ms /     2 runs   (  197.67 ms per token,     5.06 tokens per second)\n",
            "llama_perf_context_print:       total time =    2608.83 ms /   166 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 26%|██▌       | 26/100 [01:55<04:04,  3.30s/it]Llama.generate: 35 prefix-match hit, remaining 150 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3045.60 ms /   150 tokens (   20.30 ms per token,    49.25 tokens per second)\n",
            "llama_perf_context_print:        eval time =     398.71 ms /     2 runs   (  199.36 ms per token,     5.02 tokens per second)\n",
            "llama_perf_context_print:       total time =    3452.41 ms /   152 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 27%|██▋       | 27/100 [01:58<04:04,  3.35s/it]Llama.generate: 35 prefix-match hit, remaining 376 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    5036.92 ms /   376 tokens (   13.40 ms per token,    74.65 tokens per second)\n",
            "llama_perf_context_print:        eval time =     397.51 ms /     2 runs   (  198.75 ms per token,     5.03 tokens per second)\n",
            "llama_perf_context_print:       total time =    5442.88 ms /   378 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 28%|██▊       | 28/100 [02:04<04:46,  3.98s/it]Llama.generate: 35 prefix-match hit, remaining 219 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2944.72 ms /   219 tokens (   13.45 ms per token,    74.37 tokens per second)\n",
            "llama_perf_context_print:        eval time =     393.77 ms /     2 runs   (  196.88 ms per token,     5.08 tokens per second)\n",
            "llama_perf_context_print:       total time =    3346.91 ms /   221 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 29%|██▉       | 29/100 [02:07<04:29,  3.79s/it]Llama.generate: 35 prefix-match hit, remaining 469 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    7337.20 ms /   469 tokens (   15.64 ms per token,    63.92 tokens per second)\n",
            "llama_perf_context_print:        eval time =     398.74 ms /     2 runs   (  199.37 ms per token,     5.02 tokens per second)\n",
            "llama_perf_context_print:       total time =    7744.49 ms /   471 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 30%|███       | 30/100 [02:15<05:48,  4.98s/it]Llama.generate: 35 prefix-match hit, remaining 420 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    5574.01 ms /   420 tokens (   13.27 ms per token,    75.35 tokens per second)\n",
            "llama_perf_context_print:        eval time =     405.04 ms /     2 runs   (  202.52 ms per token,     4.94 tokens per second)\n",
            "llama_perf_context_print:       total time =    5987.65 ms /   422 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 31%|███       | 31/100 [02:21<06:04,  5.28s/it]Llama.generate: 35 prefix-match hit, remaining 162 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3605.49 ms /   162 tokens (   22.26 ms per token,    44.93 tokens per second)\n",
            "llama_perf_context_print:        eval time =     399.85 ms /     2 runs   (  199.93 ms per token,     5.00 tokens per second)\n",
            "llama_perf_context_print:       total time =    4013.59 ms /   164 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 32%|███▏      | 32/100 [02:25<05:33,  4.90s/it]Llama.generate: 35 prefix-match hit, remaining 421 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    5584.09 ms /   421 tokens (   13.26 ms per token,    75.39 tokens per second)\n",
            "llama_perf_context_print:        eval time =     400.75 ms /     2 runs   (  200.38 ms per token,     4.99 tokens per second)\n",
            "llama_perf_context_print:       total time =    5993.41 ms /   423 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 33%|███▎      | 33/100 [02:31<05:50,  5.23s/it]Llama.generate: 35 prefix-match hit, remaining 161 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2164.32 ms /   161 tokens (   13.44 ms per token,    74.39 tokens per second)\n",
            "llama_perf_context_print:        eval time =     393.68 ms /     2 runs   (  196.84 ms per token,     5.08 tokens per second)\n",
            "llama_perf_context_print:       total time =    2566.13 ms /   163 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 34%|███▍      | 34/100 [02:33<04:52,  4.43s/it]Llama.generate: 35 prefix-match hit, remaining 151 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3469.03 ms /   151 tokens (   22.97 ms per token,    43.53 tokens per second)\n",
            "llama_perf_context_print:        eval time =     389.03 ms /     2 runs   (  194.51 ms per token,     5.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    3866.24 ms /   153 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 35%|███▌      | 35/100 [02:37<04:37,  4.27s/it]Llama.generate: 36 prefix-match hit, remaining 73 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    1057.29 ms /    73 tokens (   14.48 ms per token,    69.04 tokens per second)\n",
            "llama_perf_context_print:        eval time =     390.21 ms /     2 runs   (  195.10 ms per token,     5.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    1455.54 ms /    75 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 36%|███▌      | 36/100 [02:39<03:39,  3.42s/it]Llama.generate: 35 prefix-match hit, remaining 297 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3937.78 ms /   297 tokens (   13.26 ms per token,    75.42 tokens per second)\n",
            "llama_perf_context_print:        eval time =     403.47 ms /     2 runs   (  201.73 ms per token,     4.96 tokens per second)\n",
            "llama_perf_context_print:       total time =    4349.74 ms /   299 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 37%|███▋      | 37/100 [02:43<03:53,  3.70s/it]Llama.generate: 35 prefix-match hit, remaining 395 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    6447.30 ms /   395 tokens (   16.32 ms per token,    61.27 tokens per second)\n",
            "llama_perf_context_print:        eval time =     395.62 ms /     2 runs   (  197.81 ms per token,     5.06 tokens per second)\n",
            "llama_perf_context_print:       total time =    6851.40 ms /   397 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 38%|███▊      | 38/100 [02:50<04:48,  4.65s/it]Llama.generate: 35 prefix-match hit, remaining 456 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    6088.32 ms /   456 tokens (   13.35 ms per token,    74.90 tokens per second)\n",
            "llama_perf_context_print:        eval time =     399.15 ms /     2 runs   (  199.57 ms per token,     5.01 tokens per second)\n",
            "llama_perf_context_print:       total time =    6496.10 ms /   458 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 39%|███▉      | 39/100 [02:57<05:17,  5.21s/it]Llama.generate: 35 prefix-match hit, remaining 416 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    6644.96 ms /   416 tokens (   15.97 ms per token,    62.60 tokens per second)\n",
            "llama_perf_context_print:        eval time =     404.17 ms /     2 runs   (  202.08 ms per token,     4.95 tokens per second)\n",
            "llama_perf_context_print:       total time =    7057.73 ms /   418 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 40%|████      | 40/100 [03:04<05:45,  5.76s/it]Llama.generate: 35 prefix-match hit, remaining 163 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2225.23 ms /   163 tokens (   13.65 ms per token,    73.25 tokens per second)\n",
            "llama_perf_context_print:        eval time =     398.96 ms /     2 runs   (  199.48 ms per token,     5.01 tokens per second)\n",
            "llama_perf_context_print:       total time =    2632.78 ms /   165 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 41%|████      | 41/100 [03:06<04:44,  4.83s/it]Llama.generate: 35 prefix-match hit, remaining 778 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =   11623.82 ms /   778 tokens (   14.94 ms per token,    66.93 tokens per second)\n",
            "llama_perf_context_print:        eval time =     408.71 ms /     2 runs   (  204.36 ms per token,     4.89 tokens per second)\n",
            "llama_perf_context_print:       total time =   12041.18 ms /   780 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 42%|████▏     | 42/100 [03:18<06:45,  6.99s/it]Llama.generate: 36 prefix-match hit, remaining 295 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3948.19 ms /   295 tokens (   13.38 ms per token,    74.72 tokens per second)\n",
            "llama_perf_context_print:        eval time =     400.46 ms /     2 runs   (  200.23 ms per token,     4.99 tokens per second)\n",
            "llama_perf_context_print:       total time =    4357.36 ms /   297 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 43%|████▎     | 43/100 [03:23<05:53,  6.20s/it]Llama.generate: 36 prefix-match hit, remaining 130 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2275.62 ms /   130 tokens (   17.50 ms per token,    57.13 tokens per second)\n",
            "llama_perf_context_print:        eval time =     389.76 ms /     2 runs   (  194.88 ms per token,     5.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    2673.48 ms /   132 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 44%|████▍     | 44/100 [03:25<04:48,  5.15s/it]Llama.generate: 35 prefix-match hit, remaining 183 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2900.52 ms /   183 tokens (   15.85 ms per token,    63.09 tokens per second)\n",
            "llama_perf_context_print:        eval time =     410.98 ms /     2 runs   (  205.49 ms per token,     4.87 tokens per second)\n",
            "llama_perf_context_print:       total time =    3319.69 ms /   185 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 45%|████▌     | 45/100 [03:29<04:12,  4.60s/it]Llama.generate: 35 prefix-match hit, remaining 150 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2048.74 ms /   150 tokens (   13.66 ms per token,    73.22 tokens per second)\n",
            "llama_perf_context_print:        eval time =     392.90 ms /     2 runs   (  196.45 ms per token,     5.09 tokens per second)\n",
            "llama_perf_context_print:       total time =    2449.78 ms /   152 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 46%|████▌     | 46/100 [03:31<03:33,  3.96s/it]Llama.generate: 35 prefix-match hit, remaining 445 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    6671.23 ms /   445 tokens (   14.99 ms per token,    66.70 tokens per second)\n",
            "llama_perf_context_print:        eval time =     404.28 ms /     2 runs   (  202.14 ms per token,     4.95 tokens per second)\n",
            "llama_perf_context_print:       total time =    7084.44 ms /   447 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 47%|████▋     | 47/100 [03:38<04:19,  4.90s/it]Llama.generate: 35 prefix-match hit, remaining 139 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    1990.74 ms /   139 tokens (   14.32 ms per token,    69.82 tokens per second)\n",
            "llama_perf_context_print:        eval time =     389.08 ms /     2 runs   (  194.54 ms per token,     5.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    2388.08 ms /   141 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 48%|████▊     | 48/100 [03:41<03:35,  4.15s/it]Llama.generate: 35 prefix-match hit, remaining 264 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3480.47 ms /   264 tokens (   13.18 ms per token,    75.85 tokens per second)\n",
            "llama_perf_context_print:        eval time =     394.29 ms /     2 runs   (  197.14 ms per token,     5.07 tokens per second)\n",
            "llama_perf_context_print:       total time =    3883.28 ms /   266 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 49%|████▉     | 49/100 [03:44<03:27,  4.07s/it]Llama.generate: 35 prefix-match hit, remaining 237 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3207.42 ms /   237 tokens (   13.53 ms per token,    73.89 tokens per second)\n",
            "llama_perf_context_print:        eval time =     400.94 ms /     2 runs   (  200.47 ms per token,     4.99 tokens per second)\n",
            "llama_perf_context_print:       total time =    3616.60 ms /   239 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 50%|█████     | 50/100 [03:48<03:16,  3.93s/it]Llama.generate: 35 prefix-match hit, remaining 123 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3123.58 ms /   123 tokens (   25.39 ms per token,    39.38 tokens per second)\n",
            "llama_perf_context_print:        eval time =     399.06 ms /     2 runs   (  199.53 ms per token,     5.01 tokens per second)\n",
            "llama_perf_context_print:       total time =    3530.83 ms /   125 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 51%|█████     | 51/100 [03:52<03:06,  3.81s/it]Llama.generate: 35 prefix-match hit, remaining 150 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2025.61 ms /   150 tokens (   13.50 ms per token,    74.05 tokens per second)\n",
            "llama_perf_context_print:        eval time =     391.55 ms /     2 runs   (  195.78 ms per token,     5.11 tokens per second)\n",
            "llama_perf_context_print:       total time =    2425.30 ms /   152 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 52%|█████▏    | 52/100 [03:54<02:43,  3.40s/it]Llama.generate: 35 prefix-match hit, remaining 64 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =     864.50 ms /    64 tokens (   13.51 ms per token,    74.03 tokens per second)\n",
            "llama_perf_context_print:        eval time =     388.44 ms /     2 runs   (  194.22 ms per token,     5.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    1261.02 ms /    66 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 53%|█████▎    | 53/100 [03:55<02:09,  2.76s/it]Llama.generate: 35 prefix-match hit, remaining 141 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    1919.40 ms /   141 tokens (   13.61 ms per token,    73.46 tokens per second)\n",
            "llama_perf_context_print:        eval time =     392.26 ms /     2 runs   (  196.13 ms per token,     5.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    2319.80 ms /   143 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 54%|█████▍    | 54/100 [03:58<02:00,  2.63s/it]Llama.generate: 35 prefix-match hit, remaining 277 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    4515.76 ms /   277 tokens (   16.30 ms per token,    61.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =     396.26 ms /     2 runs   (  198.13 ms per token,     5.05 tokens per second)\n",
            "llama_perf_context_print:       total time =    4920.15 ms /   279 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 55%|█████▌    | 55/100 [04:03<02:29,  3.32s/it]Llama.generate: 35 prefix-match hit, remaining 315 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    4325.44 ms /   315 tokens (   13.73 ms per token,    72.82 tokens per second)\n",
            "llama_perf_context_print:        eval time =     400.47 ms /     2 runs   (  200.24 ms per token,     4.99 tokens per second)\n",
            "llama_perf_context_print:       total time =    4734.63 ms /   317 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 56%|█████▌    | 56/100 [04:07<02:44,  3.74s/it]Llama.generate: 35 prefix-match hit, remaining 170 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2344.31 ms /   170 tokens (   13.79 ms per token,    72.52 tokens per second)\n",
            "llama_perf_context_print:        eval time =     397.67 ms /     2 runs   (  198.83 ms per token,     5.03 tokens per second)\n",
            "llama_perf_context_print:       total time =    2750.18 ms /   172 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 57%|█████▋    | 57/100 [04:10<02:28,  3.45s/it]Llama.generate: 35 prefix-match hit, remaining 162 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2174.11 ms /   162 tokens (   13.42 ms per token,    74.51 tokens per second)\n",
            "llama_perf_context_print:        eval time =     399.28 ms /     2 runs   (  199.64 ms per token,     5.01 tokens per second)\n",
            "llama_perf_context_print:       total time =    2581.62 ms /   164 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 58%|█████▊    | 58/100 [04:13<02:13,  3.19s/it]Llama.generate: 35 prefix-match hit, remaining 317 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    5449.09 ms /   317 tokens (   17.19 ms per token,    58.17 tokens per second)\n",
            "llama_perf_context_print:        eval time =     394.00 ms /     2 runs   (  197.00 ms per token,     5.08 tokens per second)\n",
            "llama_perf_context_print:       total time =    5851.83 ms /   319 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 59%|█████▉    | 59/100 [04:18<02:43,  3.99s/it]Llama.generate: 35 prefix-match hit, remaining 304 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    4022.51 ms /   304 tokens (   13.23 ms per token,    75.57 tokens per second)\n",
            "llama_perf_context_print:        eval time =     398.48 ms /     2 runs   (  199.24 ms per token,     5.02 tokens per second)\n",
            "llama_perf_context_print:       total time =    4429.35 ms /   306 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 60%|██████    | 60/100 [04:23<02:44,  4.12s/it]Llama.generate: 35 prefix-match hit, remaining 529 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    8196.41 ms /   529 tokens (   15.49 ms per token,    64.54 tokens per second)\n",
            "llama_perf_context_print:        eval time =     403.53 ms /     2 runs   (  201.77 ms per token,     4.96 tokens per second)\n",
            "llama_perf_context_print:       total time =    8608.54 ms /   531 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 61%|██████    | 61/100 [04:32<03:33,  5.47s/it]Llama.generate: 35 prefix-match hit, remaining 177 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2347.70 ms /   177 tokens (   13.26 ms per token,    75.39 tokens per second)\n",
            "llama_perf_context_print:        eval time =     397.06 ms /     2 runs   (  198.53 ms per token,     5.04 tokens per second)\n",
            "llama_perf_context_print:       total time =    2753.03 ms /   179 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 62%|██████▏   | 62/100 [04:34<02:56,  4.66s/it]Llama.generate: 35 prefix-match hit, remaining 183 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2514.60 ms /   183 tokens (   13.74 ms per token,    72.77 tokens per second)\n",
            "llama_perf_context_print:        eval time =     399.66 ms /     2 runs   (  199.83 ms per token,     5.00 tokens per second)\n",
            "llama_perf_context_print:       total time =    2922.61 ms /   185 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 63%|██████▎   | 63/100 [04:37<02:33,  4.14s/it]Llama.generate: 35 prefix-match hit, remaining 371 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    6076.49 ms /   371 tokens (   16.38 ms per token,    61.05 tokens per second)\n",
            "llama_perf_context_print:        eval time =     400.79 ms /     2 runs   (  200.40 ms per token,     4.99 tokens per second)\n",
            "llama_perf_context_print:       total time =    6485.70 ms /   373 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 64%|██████▍   | 64/100 [04:44<02:54,  4.84s/it]Llama.generate: 35 prefix-match hit, remaining 1272 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =   18790.10 ms /  1272 tokens (   14.77 ms per token,    67.70 tokens per second)\n",
            "llama_perf_context_print:        eval time =     209.59 ms /     1 runs   (  209.59 ms per token,     4.77 tokens per second)\n",
            "llama_perf_context_print:       total time =   19006.05 ms /  1273 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 65%|██████▌   | 65/100 [05:03<05:18,  9.10s/it]Llama.generate: 35 prefix-match hit, remaining 171 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3665.73 ms /   171 tokens (   21.44 ms per token,    46.65 tokens per second)\n",
            "llama_perf_context_print:        eval time =     396.83 ms /     2 runs   (  198.42 ms per token,     5.04 tokens per second)\n",
            "llama_perf_context_print:       total time =    4071.14 ms /   173 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 66%|██████▌   | 66/100 [05:07<04:18,  7.59s/it]Llama.generate: 35 prefix-match hit, remaining 282 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3776.67 ms /   282 tokens (   13.39 ms per token,    74.67 tokens per second)\n",
            "llama_perf_context_print:        eval time =     396.03 ms /     2 runs   (  198.02 ms per token,     5.05 tokens per second)\n",
            "llama_perf_context_print:       total time =    4181.11 ms /   284 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 67%|██████▋   | 67/100 [05:11<03:36,  6.57s/it]Llama.generate: 35 prefix-match hit, remaining 201 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2711.96 ms /   201 tokens (   13.49 ms per token,    74.12 tokens per second)\n",
            "llama_perf_context_print:        eval time =     392.71 ms /     2 runs   (  196.36 ms per token,     5.09 tokens per second)\n",
            "llama_perf_context_print:       total time =    3113.09 ms /   203 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 68%|██████▊   | 68/100 [05:14<02:57,  5.53s/it]Llama.generate: 35 prefix-match hit, remaining 359 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    5950.97 ms /   359 tokens (   16.58 ms per token,    60.33 tokens per second)\n",
            "llama_perf_context_print:        eval time =     402.30 ms /     2 runs   (  201.15 ms per token,     4.97 tokens per second)\n",
            "llama_perf_context_print:       total time =    6361.71 ms /   361 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 69%|██████▉   | 69/100 [05:20<02:59,  5.78s/it]Llama.generate: 35 prefix-match hit, remaining 147 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2003.10 ms /   147 tokens (   13.63 ms per token,    73.39 tokens per second)\n",
            "llama_perf_context_print:        eval time =     389.20 ms /     2 runs   (  194.60 ms per token,     5.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    2400.64 ms /   149 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 70%|███████   | 70/100 [05:23<02:23,  4.77s/it]Llama.generate: 35 prefix-match hit, remaining 164 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2188.57 ms /   164 tokens (   13.34 ms per token,    74.93 tokens per second)\n",
            "llama_perf_context_print:        eval time =     409.60 ms /     2 runs   (  204.80 ms per token,     4.88 tokens per second)\n",
            "llama_perf_context_print:       total time =    2606.23 ms /   166 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 71%|███████   | 71/100 [05:25<01:59,  4.12s/it]Llama.generate: 35 prefix-match hit, remaining 252 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    4155.59 ms /   252 tokens (   16.49 ms per token,    60.64 tokens per second)\n",
            "llama_perf_context_print:        eval time =     399.51 ms /     2 runs   (  199.75 ms per token,     5.01 tokens per second)\n",
            "llama_perf_context_print:       total time =    4563.44 ms /   254 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 72%|███████▏  | 72/100 [05:30<01:59,  4.26s/it]Llama.generate: 35 prefix-match hit, remaining 148 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2021.54 ms /   148 tokens (   13.66 ms per token,    73.21 tokens per second)\n",
            "llama_perf_context_print:        eval time =     389.91 ms /     2 runs   (  194.95 ms per token,     5.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    2419.60 ms /   150 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 73%|███████▎  | 73/100 [05:32<01:40,  3.71s/it]Llama.generate: 35 prefix-match hit, remaining 428 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    5681.97 ms /   428 tokens (   13.28 ms per token,    75.33 tokens per second)\n",
            "llama_perf_context_print:        eval time =     403.27 ms /     2 runs   (  201.63 ms per token,     4.96 tokens per second)\n",
            "llama_perf_context_print:       total time =    6093.73 ms /   430 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 74%|███████▍  | 74/100 [05:39<01:55,  4.42s/it]Llama.generate: 35 prefix-match hit, remaining 124 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    1681.05 ms /   124 tokens (   13.56 ms per token,    73.76 tokens per second)\n",
            "llama_perf_context_print:        eval time =     397.07 ms /     2 runs   (  198.54 ms per token,     5.04 tokens per second)\n",
            "llama_perf_context_print:       total time =    2087.07 ms /   126 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 75%|███████▌  | 75/100 [05:41<01:33,  3.72s/it]Llama.generate: 35 prefix-match hit, remaining 454 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    6804.30 ms /   454 tokens (   14.99 ms per token,    66.72 tokens per second)\n",
            "llama_perf_context_print:        eval time =     202.69 ms /     1 runs   (  202.69 ms per token,     4.93 tokens per second)\n",
            "llama_perf_context_print:       total time =    7013.67 ms /   455 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 76%|███████▌  | 76/100 [05:48<01:53,  4.71s/it]Llama.generate: 35 prefix-match hit, remaining 228 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3056.11 ms /   228 tokens (   13.40 ms per token,    74.60 tokens per second)\n",
            "llama_perf_context_print:        eval time =     398.44 ms /     2 runs   (  199.22 ms per token,     5.02 tokens per second)\n",
            "llama_perf_context_print:       total time =    3462.88 ms /   230 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 77%|███████▋  | 77/100 [05:51<01:39,  4.34s/it]Llama.generate: 35 prefix-match hit, remaining 230 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    4310.88 ms /   230 tokens (   18.74 ms per token,    53.35 tokens per second)\n",
            "llama_perf_context_print:        eval time =     397.27 ms /     2 runs   (  198.63 ms per token,     5.03 tokens per second)\n",
            "llama_perf_context_print:       total time =    4716.62 ms /   232 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 78%|███████▊  | 78/100 [05:56<01:38,  4.45s/it]Llama.generate: 35 prefix-match hit, remaining 102 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    1428.30 ms /   102 tokens (   14.00 ms per token,    71.41 tokens per second)\n",
            "llama_perf_context_print:        eval time =     394.96 ms /     2 runs   (  197.48 ms per token,     5.06 tokens per second)\n",
            "llama_perf_context_print:       total time =    1831.26 ms /   104 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 79%|███████▉  | 79/100 [05:58<01:17,  3.67s/it]Llama.generate: 35 prefix-match hit, remaining 180 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2384.12 ms /   180 tokens (   13.25 ms per token,    75.50 tokens per second)\n",
            "llama_perf_context_print:        eval time =     398.32 ms /     2 runs   (  199.16 ms per token,     5.02 tokens per second)\n",
            "llama_perf_context_print:       total time =    2790.57 ms /   182 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 80%|████████  | 80/100 [06:01<01:08,  3.41s/it]Llama.generate: 35 prefix-match hit, remaining 166 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2246.28 ms /   166 tokens (   13.53 ms per token,    73.90 tokens per second)\n",
            "llama_perf_context_print:        eval time =     397.57 ms /     2 runs   (  198.78 ms per token,     5.03 tokens per second)\n",
            "llama_perf_context_print:       total time =    2652.02 ms /   168 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 81%|████████  | 81/100 [06:03<01:00,  3.18s/it]Llama.generate: 35 prefix-match hit, remaining 271 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    4856.61 ms /   271 tokens (   17.92 ms per token,    55.80 tokens per second)\n",
            "llama_perf_context_print:        eval time =     391.93 ms /     2 runs   (  195.97 ms per token,     5.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    5256.89 ms /   273 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 82%|████████▏ | 82/100 [06:08<01:08,  3.81s/it]Llama.generate: 35 prefix-match hit, remaining 306 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    4067.03 ms /   306 tokens (   13.29 ms per token,    75.24 tokens per second)\n",
            "llama_perf_context_print:        eval time =     399.30 ms /     2 runs   (  199.65 ms per token,     5.01 tokens per second)\n",
            "llama_perf_context_print:       total time =    4474.61 ms /   308 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 83%|████████▎ | 83/100 [06:13<01:08,  4.01s/it]Llama.generate: 35 prefix-match hit, remaining 164 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2218.85 ms /   164 tokens (   13.53 ms per token,    73.91 tokens per second)\n",
            "llama_perf_context_print:        eval time =     397.74 ms /     2 runs   (  198.87 ms per token,     5.03 tokens per second)\n",
            "llama_perf_context_print:       total time =    2624.72 ms /   166 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 84%|████████▍ | 84/100 [06:16<00:57,  3.59s/it]Llama.generate: 35 prefix-match hit, remaining 421 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    6668.38 ms /   421 tokens (   15.84 ms per token,    63.13 tokens per second)\n",
            "llama_perf_context_print:        eval time =     405.87 ms /     2 runs   (  202.94 ms per token,     4.93 tokens per second)\n",
            "llama_perf_context_print:       total time =    7082.62 ms /   423 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 85%|████████▌ | 85/100 [06:23<01:09,  4.64s/it]Llama.generate: 35 prefix-match hit, remaining 226 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3011.53 ms /   226 tokens (   13.33 ms per token,    75.05 tokens per second)\n",
            "llama_perf_context_print:        eval time =     399.56 ms /     2 runs   (  199.78 ms per token,     5.01 tokens per second)\n",
            "llama_perf_context_print:       total time =    3419.40 ms /   228 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 86%|████████▌ | 86/100 [06:26<00:59,  4.28s/it]Llama.generate: 35 prefix-match hit, remaining 213 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2840.40 ms /   213 tokens (   13.34 ms per token,    74.99 tokens per second)\n",
            "llama_perf_context_print:        eval time =     391.64 ms /     2 runs   (  195.82 ms per token,     5.11 tokens per second)\n",
            "llama_perf_context_print:       total time =    3240.29 ms /   215 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 87%|████████▋ | 87/100 [06:29<00:51,  3.97s/it]Llama.generate: 35 prefix-match hit, remaining 402 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    6475.13 ms /   402 tokens (   16.11 ms per token,    62.08 tokens per second)\n",
            "llama_perf_context_print:        eval time =     199.86 ms /     1 runs   (  199.86 ms per token,     5.00 tokens per second)\n",
            "llama_perf_context_print:       total time =    6680.99 ms /   403 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 88%|████████▊ | 88/100 [06:36<00:57,  4.78s/it]Llama.generate: 35 prefix-match hit, remaining 173 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2357.23 ms /   173 tokens (   13.63 ms per token,    73.39 tokens per second)\n",
            "llama_perf_context_print:        eval time =     397.12 ms /     2 runs   (  198.56 ms per token,     5.04 tokens per second)\n",
            "llama_perf_context_print:       total time =    2762.61 ms /   175 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 89%|████████▉ | 89/100 [06:39<00:45,  4.18s/it]Llama.generate: 35 prefix-match hit, remaining 229 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3058.48 ms /   229 tokens (   13.36 ms per token,    74.87 tokens per second)\n",
            "llama_perf_context_print:        eval time =     411.44 ms /     2 runs   (  205.72 ms per token,     4.86 tokens per second)\n",
            "llama_perf_context_print:       total time =    3478.32 ms /   231 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 90%|█████████ | 90/100 [06:42<00:39,  3.97s/it]Llama.generate: 35 prefix-match hit, remaining 155 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3406.41 ms /   155 tokens (   21.98 ms per token,    45.50 tokens per second)\n",
            "llama_perf_context_print:        eval time =     394.52 ms /     2 runs   (  197.26 ms per token,     5.07 tokens per second)\n",
            "llama_perf_context_print:       total time =    3809.06 ms /   157 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 91%|█████████ | 91/100 [06:46<00:35,  3.92s/it]Llama.generate: 35 prefix-match hit, remaining 260 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3483.13 ms /   260 tokens (   13.40 ms per token,    74.65 tokens per second)\n",
            "llama_perf_context_print:        eval time =     403.57 ms /     2 runs   (  201.78 ms per token,     4.96 tokens per second)\n",
            "llama_perf_context_print:       total time =    3895.13 ms /   262 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 92%|█████████▏| 92/100 [06:50<00:31,  3.92s/it]Llama.generate: 35 prefix-match hit, remaining 302 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    4076.47 ms /   302 tokens (   13.50 ms per token,    74.08 tokens per second)\n",
            "llama_perf_context_print:        eval time =     401.31 ms /     2 runs   (  200.66 ms per token,     4.98 tokens per second)\n",
            "llama_perf_context_print:       total time =    4486.14 ms /   304 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 93%|█████████▎| 93/100 [06:54<00:28,  4.09s/it]Llama.generate: 35 prefix-match hit, remaining 272 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    4799.43 ms /   272 tokens (   17.64 ms per token,    56.67 tokens per second)\n",
            "llama_perf_context_print:        eval time =     395.07 ms /     2 runs   (  197.54 ms per token,     5.06 tokens per second)\n",
            "llama_perf_context_print:       total time =    5203.02 ms /   274 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 94%|█████████▍| 94/100 [07:00<00:26,  4.42s/it]Llama.generate: 35 prefix-match hit, remaining 353 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    4704.45 ms /   353 tokens (   13.33 ms per token,    75.04 tokens per second)\n",
            "llama_perf_context_print:        eval time =     405.47 ms /     2 runs   (  202.73 ms per token,     4.93 tokens per second)\n",
            "llama_perf_context_print:       total time =    5118.33 ms /   355 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 95%|█████████▌| 95/100 [07:05<00:23,  4.63s/it]Llama.generate: 35 prefix-match hit, remaining 310 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    5349.49 ms /   310 tokens (   17.26 ms per token,    57.95 tokens per second)\n",
            "llama_perf_context_print:        eval time =     398.51 ms /     2 runs   (  199.26 ms per token,     5.02 tokens per second)\n",
            "llama_perf_context_print:       total time =    5756.80 ms /   312 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 96%|█████████▌| 96/100 [07:11<00:19,  4.97s/it]Llama.generate: 35 prefix-match hit, remaining 171 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2347.02 ms /   171 tokens (   13.73 ms per token,    72.86 tokens per second)\n",
            "llama_perf_context_print:        eval time =     395.21 ms /     2 runs   (  197.60 ms per token,     5.06 tokens per second)\n",
            "llama_perf_context_print:       total time =    2750.44 ms /   173 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 97%|█████████▋| 97/100 [07:13<00:12,  4.31s/it]Llama.generate: 35 prefix-match hit, remaining 165 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2245.17 ms /   165 tokens (   13.61 ms per token,    73.49 tokens per second)\n",
            "llama_perf_context_print:        eval time =     400.25 ms /     2 runs   (  200.13 ms per token,     5.00 tokens per second)\n",
            "llama_perf_context_print:       total time =    2653.55 ms /   167 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 98%|█████████▊| 98/100 [07:16<00:07,  3.81s/it]Llama.generate: 35 prefix-match hit, remaining 169 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2275.85 ms /   169 tokens (   13.47 ms per token,    74.26 tokens per second)\n",
            "llama_perf_context_print:        eval time =     398.20 ms /     2 runs   (  199.10 ms per token,     5.02 tokens per second)\n",
            "llama_perf_context_print:       total time =    2682.41 ms /   171 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            " 99%|█████████▉| 99/100 [07:19<00:03,  3.48s/it]Llama.generate: 35 prefix-match hit, remaining 183 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3835.57 ms /   183 tokens (   20.96 ms per token,    47.71 tokens per second)\n",
            "llama_perf_context_print:        eval time =     395.66 ms /     2 runs   (  197.83 ms per token,     5.05 tokens per second)\n",
            "llama_perf_context_print:       total time =    4239.29 ms /   185 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            "100%|██████████| 100/100 [07:23<00:00,  4.43s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for zero_shot: 0.87\n",
            "Running evaluation for strategy: few_shot...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/100 [00:00<?, ?it/s]Llama.generate: 32 prefix-match hit, remaining 784 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =   11613.24 ms /   784 tokens (   14.81 ms per token,    67.51 tokens per second)\n",
            "llama_perf_context_print:        eval time =     205.52 ms /     1 runs   (  205.52 ms per token,     4.87 tokens per second)\n",
            "llama_perf_context_print:       total time =   11824.74 ms /   785 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "  1%|          | 1/100 [00:11<19:31, 11.83s/it]Llama.generate: 570 prefix-match hit, remaining 385 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    5340.88 ms /   385 tokens (   13.87 ms per token,    72.09 tokens per second)\n",
            "llama_perf_context_print:        eval time =     206.54 ms /     1 runs   (  206.54 ms per token,     4.84 tokens per second)\n",
            "llama_perf_context_print:       total time =    5553.44 ms /   386 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "  2%|▏         | 2/100 [00:17<13:18,  8.14s/it]Llama.generate: 570 prefix-match hit, remaining 679 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =   10636.26 ms /   679 tokens (   15.66 ms per token,    63.84 tokens per second)\n",
            "llama_perf_context_print:        eval time =     209.07 ms /     1 runs   (  209.07 ms per token,     4.78 tokens per second)\n",
            "llama_perf_context_print:       total time =   10851.70 ms /   680 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "  3%|▎         | 3/100 [00:28<15:10,  9.39s/it]Llama.generate: 570 prefix-match hit, remaining 352 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    4875.56 ms /   352 tokens (   13.85 ms per token,    72.20 tokens per second)\n",
            "llama_perf_context_print:        eval time =     206.80 ms /     1 runs   (  206.80 ms per token,     4.84 tokens per second)\n",
            "llama_perf_context_print:       total time =    5088.47 ms /   353 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "  4%|▍         | 4/100 [00:33<12:18,  7.69s/it]Llama.generate: 570 prefix-match hit, remaining 103 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2552.47 ms /   103 tokens (   24.78 ms per token,    40.35 tokens per second)\n",
            "llama_perf_context_print:        eval time =     203.50 ms /     1 runs   (  203.50 ms per token,     4.91 tokens per second)\n",
            "llama_perf_context_print:       total time =    2761.59 ms /   104 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "  5%|▌         | 5/100 [00:36<09:22,  5.92s/it]Llama.generate: 570 prefix-match hit, remaining 187 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2873.32 ms /   187 tokens (   15.37 ms per token,    65.08 tokens per second)\n",
            "llama_perf_context_print:        eval time =     204.13 ms /     1 runs   (  204.13 ms per token,     4.90 tokens per second)\n",
            "llama_perf_context_print:       total time =    3083.53 ms /   188 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "  6%|▌         | 6/100 [00:39<07:45,  4.96s/it]Llama.generate: 570 prefix-match hit, remaining 129 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    1823.89 ms /   129 tokens (   14.14 ms per token,    70.73 tokens per second)\n",
            "llama_perf_context_print:        eval time =     203.69 ms /     1 runs   (  203.69 ms per token,     4.91 tokens per second)\n",
            "llama_perf_context_print:       total time =    2033.24 ms /   130 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "  7%|▋         | 7/100 [00:41<06:12,  4.00s/it]Llama.generate: 570 prefix-match hit, remaining 166 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2376.01 ms /   166 tokens (   14.31 ms per token,    69.87 tokens per second)\n",
            "llama_perf_context_print:        eval time =     204.98 ms /     1 runs   (  204.98 ms per token,     4.88 tokens per second)\n",
            "llama_perf_context_print:       total time =    2586.72 ms /   167 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "  8%|▊         | 8/100 [00:43<05:27,  3.55s/it]Llama.generate: 570 prefix-match hit, remaining 291 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    5258.10 ms /   291 tokens (   18.07 ms per token,    55.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =     212.29 ms /     1 runs   (  212.29 ms per token,     4.71 tokens per second)\n",
            "llama_perf_context_print:       total time =    5476.23 ms /   292 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "  9%|▉         | 9/100 [00:49<06:18,  4.16s/it]Llama.generate: 570 prefix-match hit, remaining 536 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    7528.40 ms /   536 tokens (   14.05 ms per token,    71.20 tokens per second)\n",
            "llama_perf_context_print:        eval time =     210.51 ms /     1 runs   (  210.51 ms per token,     4.75 tokens per second)\n",
            "llama_perf_context_print:       total time =    7745.03 ms /   537 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 10%|█         | 10/100 [00:57<07:54,  5.27s/it]Llama.generate: 570 prefix-match hit, remaining 627 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    9916.65 ms /   627 tokens (   15.82 ms per token,    63.23 tokens per second)\n",
            "llama_perf_context_print:        eval time =     210.56 ms /     1 runs   (  210.56 ms per token,     4.75 tokens per second)\n",
            "llama_perf_context_print:       total time =   10133.45 ms /   628 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 11%|█         | 11/100 [01:07<10:01,  6.76s/it]Llama.generate: 571 prefix-match hit, remaining 164 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2276.16 ms /   164 tokens (   13.88 ms per token,    72.05 tokens per second)\n",
            "llama_perf_context_print:        eval time =     205.01 ms /     1 runs   (  205.01 ms per token,     4.88 tokens per second)\n",
            "llama_perf_context_print:       total time =    2487.10 ms /   165 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 12%|█▏        | 12/100 [01:09<08:00,  5.46s/it]Llama.generate: 570 prefix-match hit, remaining 247 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    4779.13 ms /   247 tokens (   19.35 ms per token,    51.68 tokens per second)\n",
            "llama_perf_context_print:        eval time =     204.81 ms /     1 runs   (  204.81 ms per token,     4.88 tokens per second)\n",
            "llama_perf_context_print:       total time =    4989.77 ms /   248 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 13%|█▎        | 13/100 [01:14<07:42,  5.32s/it]Llama.generate: 570 prefix-match hit, remaining 247 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3456.55 ms /   247 tokens (   13.99 ms per token,    71.46 tokens per second)\n",
            "llama_perf_context_print:        eval time =     204.06 ms /     1 runs   (  204.06 ms per token,     4.90 tokens per second)\n",
            "llama_perf_context_print:       total time =    3666.42 ms /   248 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 14%|█▍        | 14/100 [01:18<06:54,  4.82s/it]Llama.generate: 570 prefix-match hit, remaining 274 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3793.05 ms /   274 tokens (   13.84 ms per token,    72.24 tokens per second)\n",
            "llama_perf_context_print:        eval time =     207.44 ms /     1 runs   (  207.44 ms per token,     4.82 tokens per second)\n",
            "llama_perf_context_print:       total time =    4006.37 ms /   275 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 15%|█▌        | 15/100 [01:22<06:29,  4.58s/it]Llama.generate: 570 prefix-match hit, remaining 167 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3364.32 ms /   167 tokens (   20.15 ms per token,    49.64 tokens per second)\n",
            "llama_perf_context_print:        eval time =     211.20 ms /     1 runs   (  211.20 ms per token,     4.73 tokens per second)\n",
            "llama_perf_context_print:       total time =    3581.38 ms /   168 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 16%|█▌        | 16/100 [01:26<05:59,  4.28s/it]Llama.generate: 570 prefix-match hit, remaining 265 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3965.46 ms /   265 tokens (   14.96 ms per token,    66.83 tokens per second)\n",
            "llama_perf_context_print:        eval time =     208.13 ms /     1 runs   (  208.13 ms per token,     4.80 tokens per second)\n",
            "llama_perf_context_print:       total time =    4179.62 ms /   266 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 17%|█▋        | 17/100 [01:30<05:53,  4.25s/it]Llama.generate: 570 prefix-match hit, remaining 263 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3683.84 ms /   263 tokens (   14.01 ms per token,    71.39 tokens per second)\n",
            "llama_perf_context_print:        eval time =     219.85 ms /     1 runs   (  219.85 ms per token,     4.55 tokens per second)\n",
            "llama_perf_context_print:       total time =    3909.66 ms /   264 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 18%|█▊        | 18/100 [01:34<05:40,  4.15s/it]Llama.generate: 570 prefix-match hit, remaining 187 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3211.65 ms /   187 tokens (   17.17 ms per token,    58.23 tokens per second)\n",
            "llama_perf_context_print:        eval time =     204.46 ms /     1 runs   (  204.46 ms per token,     4.89 tokens per second)\n",
            "llama_perf_context_print:       total time =    3421.84 ms /   188 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 19%|█▉        | 19/100 [01:37<05:18,  3.94s/it]Llama.generate: 570 prefix-match hit, remaining 209 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3523.68 ms /   209 tokens (   16.86 ms per token,    59.31 tokens per second)\n",
            "llama_perf_context_print:        eval time =     206.03 ms /     1 runs   (  206.03 ms per token,     4.85 tokens per second)\n",
            "llama_perf_context_print:       total time =    3735.56 ms /   210 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 20%|██        | 20/100 [01:41<05:10,  3.88s/it]Llama.generate: 570 prefix-match hit, remaining 87 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    1343.29 ms /    87 tokens (   15.44 ms per token,    64.77 tokens per second)\n",
            "llama_perf_context_print:        eval time =     206.22 ms /     1 runs   (  206.22 ms per token,     4.85 tokens per second)\n",
            "llama_perf_context_print:       total time =    1555.21 ms /    88 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 21%|██        | 21/100 [01:42<04:11,  3.18s/it]Llama.generate: 570 prefix-match hit, remaining 370 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    5138.04 ms /   370 tokens (   13.89 ms per token,    72.01 tokens per second)\n",
            "llama_perf_context_print:        eval time =     205.53 ms /     1 runs   (  205.53 ms per token,     4.87 tokens per second)\n",
            "llama_perf_context_print:       total time =    5349.71 ms /   371 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 22%|██▏       | 22/100 [01:48<04:59,  3.84s/it]Llama.generate: 570 prefix-match hit, remaining 189 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    4002.52 ms /   189 tokens (   21.18 ms per token,    47.22 tokens per second)\n",
            "llama_perf_context_print:        eval time =     203.75 ms /     1 runs   (  203.75 ms per token,     4.91 tokens per second)\n",
            "llama_perf_context_print:       total time =    4212.07 ms /   190 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 23%|██▎       | 23/100 [01:52<05:04,  3.95s/it]Llama.generate: 570 prefix-match hit, remaining 145 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2042.12 ms /   145 tokens (   14.08 ms per token,    71.00 tokens per second)\n",
            "llama_perf_context_print:        eval time =     205.97 ms /     1 runs   (  205.97 ms per token,     4.86 tokens per second)\n",
            "llama_perf_context_print:       total time =    2253.82 ms /   146 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 24%|██▍       | 24/100 [01:54<04:21,  3.44s/it]Llama.generate: 570 prefix-match hit, remaining 256 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3530.42 ms /   256 tokens (   13.79 ms per token,    72.51 tokens per second)\n",
            "llama_perf_context_print:        eval time =     204.54 ms /     1 runs   (  204.54 ms per token,     4.89 tokens per second)\n",
            "llama_perf_context_print:       total time =    3741.21 ms /   257 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 25%|██▌       | 25/100 [01:58<04:25,  3.54s/it]Llama.generate: 570 prefix-match hit, remaining 164 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2343.73 ms /   164 tokens (   14.29 ms per token,    69.97 tokens per second)\n",
            "llama_perf_context_print:        eval time =     204.30 ms /     1 runs   (  204.30 ms per token,     4.89 tokens per second)\n",
            "llama_perf_context_print:       total time =    2553.75 ms /   165 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 26%|██▌       | 26/100 [02:00<04:00,  3.24s/it]Llama.generate: 570 prefix-match hit, remaining 150 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3515.84 ms /   150 tokens (   23.44 ms per token,    42.66 tokens per second)\n",
            "llama_perf_context_print:        eval time =     206.68 ms /     1 runs   (  206.68 ms per token,     4.84 tokens per second)\n",
            "llama_perf_context_print:       total time =    3728.35 ms /   151 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 27%|██▋       | 27/100 [02:04<04:07,  3.39s/it]Llama.generate: 570 prefix-match hit, remaining 376 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    5212.81 ms /   376 tokens (   13.86 ms per token,    72.13 tokens per second)\n",
            "llama_perf_context_print:        eval time =     206.48 ms /     1 runs   (  206.48 ms per token,     4.84 tokens per second)\n",
            "llama_perf_context_print:       total time =    5425.23 ms /   377 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 28%|██▊       | 28/100 [02:10<04:48,  4.00s/it]Llama.generate: 570 prefix-match hit, remaining 219 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3095.42 ms /   219 tokens (   14.13 ms per token,    70.75 tokens per second)\n",
            "llama_perf_context_print:        eval time =     206.14 ms /     1 runs   (  206.14 ms per token,     4.85 tokens per second)\n",
            "llama_perf_context_print:       total time =    3307.44 ms /   220 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 29%|██▉       | 29/100 [02:13<04:29,  3.80s/it]Llama.generate: 570 prefix-match hit, remaining 469 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    7632.04 ms /   469 tokens (   16.27 ms per token,    61.45 tokens per second)\n",
            "llama_perf_context_print:        eval time =     209.46 ms /     1 runs   (  209.46 ms per token,     4.77 tokens per second)\n",
            "llama_perf_context_print:       total time =    7847.66 ms /   470 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 30%|███       | 30/100 [02:21<05:51,  5.02s/it]Llama.generate: 570 prefix-match hit, remaining 420 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    6586.63 ms /   420 tokens (   15.68 ms per token,    63.77 tokens per second)\n",
            "llama_perf_context_print:        eval time =     209.18 ms /     1 runs   (  209.18 ms per token,     4.78 tokens per second)\n",
            "llama_perf_context_print:       total time =    6801.96 ms /   421 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 31%|███       | 31/100 [02:28<06:23,  5.55s/it]Llama.generate: 570 prefix-match hit, remaining 162 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2562.29 ms /   162 tokens (   15.82 ms per token,    63.22 tokens per second)\n",
            "llama_perf_context_print:        eval time =     206.17 ms /     1 runs   (  206.17 ms per token,     4.85 tokens per second)\n",
            "llama_perf_context_print:       total time =    2774.53 ms /   163 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 32%|███▏      | 32/100 [02:30<05:21,  4.72s/it]Llama.generate: 570 prefix-match hit, remaining 421 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    5908.58 ms /   421 tokens (   14.03 ms per token,    71.25 tokens per second)\n",
            "llama_perf_context_print:        eval time =     206.96 ms /     1 runs   (  206.96 ms per token,     4.83 tokens per second)\n",
            "llama_perf_context_print:       total time =    6121.81 ms /   422 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 33%|███▎      | 33/100 [02:37<05:44,  5.14s/it]Llama.generate: 570 prefix-match hit, remaining 161 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3241.78 ms /   161 tokens (   20.14 ms per token,    49.66 tokens per second)\n",
            "llama_perf_context_print:        eval time =     204.90 ms /     1 runs   (  204.90 ms per token,     4.88 tokens per second)\n",
            "llama_perf_context_print:       total time =    3452.44 ms /   162 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 34%|███▍      | 34/100 [02:40<05:06,  4.64s/it]Llama.generate: 570 prefix-match hit, remaining 151 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2302.37 ms /   151 tokens (   15.25 ms per token,    65.58 tokens per second)\n",
            "llama_perf_context_print:        eval time =     216.03 ms /     1 runs   (  216.03 ms per token,     4.63 tokens per second)\n",
            "llama_perf_context_print:       total time =    2524.22 ms /   152 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 35%|███▌      | 35/100 [02:43<04:20,  4.01s/it]Llama.generate: 571 prefix-match hit, remaining 73 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    1097.56 ms /    73 tokens (   15.04 ms per token,    66.51 tokens per second)\n",
            "llama_perf_context_print:        eval time =     202.46 ms /     1 runs   (  202.46 ms per token,     4.94 tokens per second)\n",
            "llama_perf_context_print:       total time =    1305.65 ms /    74 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 36%|███▌      | 36/100 [02:44<03:24,  3.20s/it]Llama.generate: 570 prefix-match hit, remaining 297 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    4181.69 ms /   297 tokens (   14.08 ms per token,    71.02 tokens per second)\n",
            "llama_perf_context_print:        eval time =     206.07 ms /     1 runs   (  206.07 ms per token,     4.85 tokens per second)\n",
            "llama_perf_context_print:       total time =    4393.67 ms /   298 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 37%|███▋      | 37/100 [02:48<03:44,  3.56s/it]Llama.generate: 570 prefix-match hit, remaining 395 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    6643.90 ms /   395 tokens (   16.82 ms per token,    59.45 tokens per second)\n",
            "llama_perf_context_print:        eval time =     209.34 ms /     1 runs   (  209.34 ms per token,     4.78 tokens per second)\n",
            "llama_perf_context_print:       total time =    6859.85 ms /   396 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 38%|███▊      | 38/100 [02:55<04:42,  4.55s/it]Llama.generate: 570 prefix-match hit, remaining 456 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    6427.18 ms /   456 tokens (   14.09 ms per token,    70.95 tokens per second)\n",
            "llama_perf_context_print:        eval time =     208.60 ms /     1 runs   (  208.60 ms per token,     4.79 tokens per second)\n",
            "llama_perf_context_print:       total time =    6641.96 ms /   457 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 39%|███▉      | 39/100 [03:02<05:16,  5.18s/it]Llama.generate: 570 prefix-match hit, remaining 416 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    6911.10 ms /   416 tokens (   16.61 ms per token,    60.19 tokens per second)\n",
            "llama_perf_context_print:        eval time =     208.83 ms /     1 runs   (  208.83 ms per token,     4.79 tokens per second)\n",
            "llama_perf_context_print:       total time =    7126.15 ms /   417 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 40%|████      | 40/100 [03:09<05:46,  5.77s/it]Llama.generate: 570 prefix-match hit, remaining 163 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2311.72 ms /   163 tokens (   14.18 ms per token,    70.51 tokens per second)\n",
            "llama_perf_context_print:        eval time =     204.96 ms /     1 runs   (  204.96 ms per token,     4.88 tokens per second)\n",
            "llama_perf_context_print:       total time =    2522.45 ms /   164 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 41%|████      | 41/100 [03:11<04:42,  4.80s/it]Llama.generate: 570 prefix-match hit, remaining 778 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =   12085.97 ms /   778 tokens (   15.53 ms per token,    64.37 tokens per second)\n",
            "llama_perf_context_print:        eval time =     209.22 ms /     1 runs   (  209.22 ms per token,     4.78 tokens per second)\n",
            "llama_perf_context_print:       total time =   12301.30 ms /   779 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 42%|████▏     | 42/100 [03:24<06:48,  7.05s/it]Llama.generate: 571 prefix-match hit, remaining 295 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    4175.03 ms /   295 tokens (   14.15 ms per token,    70.66 tokens per second)\n",
            "llama_perf_context_print:        eval time =     205.64 ms /     1 runs   (  205.64 ms per token,     4.86 tokens per second)\n",
            "llama_perf_context_print:       total time =    4386.84 ms /   296 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 43%|████▎     | 43/100 [03:28<05:56,  6.25s/it]Llama.generate: 571 prefix-match hit, remaining 130 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3276.09 ms /   130 tokens (   25.20 ms per token,    39.68 tokens per second)\n",
            "llama_perf_context_print:        eval time =     202.73 ms /     1 runs   (  202.73 ms per token,     4.93 tokens per second)\n",
            "llama_perf_context_print:       total time =    3484.82 ms /   131 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 44%|████▍     | 44/100 [03:32<05:03,  5.43s/it]Llama.generate: 570 prefix-match hit, remaining 183 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2609.85 ms /   183 tokens (   14.26 ms per token,    70.12 tokens per second)\n",
            "llama_perf_context_print:        eval time =     202.47 ms /     1 runs   (  202.47 ms per token,     4.94 tokens per second)\n",
            "llama_perf_context_print:       total time =    2818.07 ms /   184 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 45%|████▌     | 45/100 [03:34<04:15,  4.65s/it]Llama.generate: 570 prefix-match hit, remaining 150 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2124.83 ms /   150 tokens (   14.17 ms per token,    70.59 tokens per second)\n",
            "llama_perf_context_print:        eval time =     204.97 ms /     1 runs   (  204.97 ms per token,     4.88 tokens per second)\n",
            "llama_perf_context_print:       total time =    2335.53 ms /   151 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 46%|████▌     | 46/100 [03:37<03:33,  3.95s/it]Llama.generate: 570 prefix-match hit, remaining 445 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    7344.96 ms /   445 tokens (   16.51 ms per token,    60.59 tokens per second)\n",
            "llama_perf_context_print:        eval time =     205.03 ms /     1 runs   (  205.03 ms per token,     4.88 tokens per second)\n",
            "llama_perf_context_print:       total time =    7556.21 ms /   446 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 47%|████▋     | 47/100 [03:44<04:27,  5.04s/it]Llama.generate: 570 prefix-match hit, remaining 139 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2023.90 ms /   139 tokens (   14.56 ms per token,    68.68 tokens per second)\n",
            "llama_perf_context_print:        eval time =     205.50 ms /     1 runs   (  205.50 ms per token,     4.87 tokens per second)\n",
            "llama_perf_context_print:       total time =    2235.19 ms /   140 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 48%|████▊     | 48/100 [03:47<03:38,  4.20s/it]Llama.generate: 570 prefix-match hit, remaining 264 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3673.97 ms /   264 tokens (   13.92 ms per token,    71.86 tokens per second)\n",
            "llama_perf_context_print:        eval time =     206.99 ms /     1 runs   (  206.99 ms per token,     4.83 tokens per second)\n",
            "llama_perf_context_print:       total time =    3886.82 ms /   265 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 49%|████▉     | 49/100 [03:51<03:29,  4.11s/it]Llama.generate: 570 prefix-match hit, remaining 237 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    4274.22 ms /   237 tokens (   18.03 ms per token,    55.45 tokens per second)\n",
            "llama_perf_context_print:        eval time =     209.87 ms /     1 runs   (  209.87 ms per token,     4.76 tokens per second)\n",
            "llama_perf_context_print:       total time =    4489.96 ms /   238 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 50%|█████     | 50/100 [03:55<03:31,  4.23s/it]Llama.generate: 570 prefix-match hit, remaining 123 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2060.87 ms /   123 tokens (   16.76 ms per token,    59.68 tokens per second)\n",
            "llama_perf_context_print:        eval time =     202.02 ms /     1 runs   (  202.02 ms per token,     4.95 tokens per second)\n",
            "llama_perf_context_print:       total time =    2268.52 ms /   124 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 51%|█████     | 51/100 [03:57<02:58,  3.64s/it]Llama.generate: 570 prefix-match hit, remaining 150 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2123.39 ms /   150 tokens (   14.16 ms per token,    70.64 tokens per second)\n",
            "llama_perf_context_print:        eval time =     205.10 ms /     1 runs   (  205.10 ms per token,     4.88 tokens per second)\n",
            "llama_perf_context_print:       total time =    2334.15 ms /   151 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 52%|█████▏    | 52/100 [04:00<02:36,  3.25s/it]Llama.generate: 570 prefix-match hit, remaining 64 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =     922.56 ms /    64 tokens (   14.41 ms per token,    69.37 tokens per second)\n",
            "llama_perf_context_print:        eval time =     202.84 ms /     1 runs   (  202.84 ms per token,     4.93 tokens per second)\n",
            "llama_perf_context_print:       total time =    1130.98 ms /    65 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 53%|█████▎    | 53/100 [04:01<02:03,  2.62s/it]Llama.generate: 570 prefix-match hit, remaining 141 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2084.60 ms /   141 tokens (   14.78 ms per token,    67.64 tokens per second)\n",
            "llama_perf_context_print:        eval time =     204.83 ms /     1 runs   (  204.83 ms per token,     4.88 tokens per second)\n",
            "llama_perf_context_print:       total time =    2295.12 ms /   142 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 54%|█████▍    | 54/100 [04:03<01:56,  2.52s/it]Llama.generate: 570 prefix-match hit, remaining 277 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    5024.43 ms /   277 tokens (   18.14 ms per token,    55.13 tokens per second)\n",
            "llama_perf_context_print:        eval time =     206.36 ms /     1 runs   (  206.36 ms per token,     4.85 tokens per second)\n",
            "llama_perf_context_print:       total time =    5236.66 ms /   278 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 55%|█████▌    | 55/100 [04:08<02:30,  3.34s/it]Llama.generate: 570 prefix-match hit, remaining 315 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    4412.83 ms /   315 tokens (   14.01 ms per token,    71.38 tokens per second)\n",
            "llama_perf_context_print:        eval time =     204.37 ms /     1 runs   (  204.37 ms per token,     4.89 tokens per second)\n",
            "llama_perf_context_print:       total time =    4623.15 ms /   316 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 56%|█████▌    | 56/100 [04:13<02:43,  3.73s/it]Llama.generate: 570 prefix-match hit, remaining 170 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2406.78 ms /   170 tokens (   14.16 ms per token,    70.63 tokens per second)\n",
            "llama_perf_context_print:        eval time =     202.84 ms /     1 runs   (  202.84 ms per token,     4.93 tokens per second)\n",
            "llama_perf_context_print:       total time =    2615.45 ms /   171 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 57%|█████▋    | 57/100 [04:16<02:26,  3.40s/it]Llama.generate: 570 prefix-match hit, remaining 162 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2282.22 ms /   162 tokens (   14.09 ms per token,    70.98 tokens per second)\n",
            "llama_perf_context_print:        eval time =     208.72 ms /     1 runs   (  208.72 ms per token,     4.79 tokens per second)\n",
            "llama_perf_context_print:       total time =    2496.77 ms /   163 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 58%|█████▊    | 58/100 [04:18<02:11,  3.13s/it]Llama.generate: 570 prefix-match hit, remaining 317 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    5456.04 ms /   317 tokens (   17.21 ms per token,    58.10 tokens per second)\n",
            "llama_perf_context_print:        eval time =     204.32 ms /     1 runs   (  204.32 ms per token,     4.89 tokens per second)\n",
            "llama_perf_context_print:       total time =    5666.37 ms /   318 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 59%|█████▉    | 59/100 [04:24<02:39,  3.89s/it]Llama.generate: 570 prefix-match hit, remaining 304 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    4202.66 ms /   304 tokens (   13.82 ms per token,    72.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =     205.56 ms /     1 runs   (  205.56 ms per token,     4.86 tokens per second)\n",
            "llama_perf_context_print:       total time =    4414.21 ms /   305 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 60%|██████    | 60/100 [04:28<02:42,  4.05s/it]Llama.generate: 570 prefix-match hit, remaining 529 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    8460.83 ms /   529 tokens (   15.99 ms per token,    62.52 tokens per second)\n",
            "llama_perf_context_print:        eval time =     209.02 ms /     1 runs   (  209.02 ms per token,     4.78 tokens per second)\n",
            "llama_perf_context_print:       total time =    8676.11 ms /   530 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 61%|██████    | 61/100 [04:37<03:32,  5.44s/it]Llama.generate: 570 prefix-match hit, remaining 177 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2480.63 ms /   177 tokens (   14.01 ms per token,    71.35 tokens per second)\n",
            "llama_perf_context_print:        eval time =     203.03 ms /     1 runs   (  203.03 ms per token,     4.93 tokens per second)\n",
            "llama_perf_context_print:       total time =    2689.55 ms /   178 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 62%|██████▏   | 62/100 [04:40<02:55,  4.62s/it]Llama.generate: 570 prefix-match hit, remaining 183 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2579.29 ms /   183 tokens (   14.09 ms per token,    70.95 tokens per second)\n",
            "llama_perf_context_print:        eval time =     204.20 ms /     1 runs   (  204.20 ms per token,     4.90 tokens per second)\n",
            "llama_perf_context_print:       total time =    2789.60 ms /   184 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 63%|██████▎   | 63/100 [04:42<02:30,  4.07s/it]Llama.generate: 570 prefix-match hit, remaining 371 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    6295.77 ms /   371 tokens (   16.97 ms per token,    58.93 tokens per second)\n",
            "llama_perf_context_print:        eval time =     204.11 ms /     1 runs   (  204.11 ms per token,     4.90 tokens per second)\n",
            "llama_perf_context_print:       total time =    6505.84 ms /   372 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 64%|██████▍   | 64/100 [04:49<02:52,  4.80s/it]Llama.generate: 570 prefix-match hit, remaining 1272 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =   20376.93 ms /  1272 tokens (   16.02 ms per token,    62.42 tokens per second)\n",
            "llama_perf_context_print:        eval time =     213.75 ms /     1 runs   (  213.75 ms per token,     4.68 tokens per second)\n",
            "llama_perf_context_print:       total time =   20597.04 ms /  1273 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 65%|██████▌   | 65/100 [05:09<05:34,  9.55s/it]Llama.generate: 570 prefix-match hit, remaining 171 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2741.95 ms /   171 tokens (   16.03 ms per token,    62.36 tokens per second)\n",
            "llama_perf_context_print:        eval time =     203.24 ms /     1 runs   (  203.24 ms per token,     4.92 tokens per second)\n",
            "llama_perf_context_print:       total time =    2951.30 ms /   172 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 66%|██████▌   | 66/100 [05:12<04:17,  7.57s/it]Llama.generate: 570 prefix-match hit, remaining 282 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3945.32 ms /   282 tokens (   13.99 ms per token,    71.48 tokens per second)\n",
            "llama_perf_context_print:        eval time =     207.99 ms /     1 runs   (  207.99 ms per token,     4.81 tokens per second)\n",
            "llama_perf_context_print:       total time =    4159.20 ms /   283 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 67%|██████▋   | 67/100 [05:17<03:36,  6.55s/it]Llama.generate: 570 prefix-match hit, remaining 201 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2834.49 ms /   201 tokens (   14.10 ms per token,    70.91 tokens per second)\n",
            "llama_perf_context_print:        eval time =     205.84 ms /     1 runs   (  205.84 ms per token,     4.86 tokens per second)\n",
            "llama_perf_context_print:       total time =    3046.21 ms /   202 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 68%|██████▊   | 68/100 [05:20<02:56,  5.50s/it]Llama.generate: 570 prefix-match hit, remaining 359 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    6220.45 ms /   359 tokens (   17.33 ms per token,    57.71 tokens per second)\n",
            "llama_perf_context_print:        eval time =     207.11 ms /     1 runs   (  207.11 ms per token,     4.83 tokens per second)\n",
            "llama_perf_context_print:       total time =    6433.59 ms /   360 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 69%|██████▉   | 69/100 [05:26<02:59,  5.78s/it]Llama.generate: 570 prefix-match hit, remaining 147 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2107.19 ms /   147 tokens (   14.33 ms per token,    69.76 tokens per second)\n",
            "llama_perf_context_print:        eval time =     207.38 ms /     1 runs   (  207.38 ms per token,     4.82 tokens per second)\n",
            "llama_perf_context_print:       total time =    2320.41 ms /   148 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 70%|███████   | 70/100 [05:28<02:22,  4.75s/it]Llama.generate: 570 prefix-match hit, remaining 164 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2287.84 ms /   164 tokens (   13.95 ms per token,    71.68 tokens per second)\n",
            "llama_perf_context_print:        eval time =     205.06 ms /     1 runs   (  205.06 ms per token,     4.88 tokens per second)\n",
            "llama_perf_context_print:       total time =    2498.68 ms /   165 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 71%|███████   | 71/100 [05:31<01:58,  4.07s/it]Llama.generate: 570 prefix-match hit, remaining 252 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    4806.26 ms /   252 tokens (   19.07 ms per token,    52.43 tokens per second)\n",
            "llama_perf_context_print:        eval time =     205.12 ms /     1 runs   (  205.12 ms per token,     4.88 tokens per second)\n",
            "llama_perf_context_print:       total time =    5017.42 ms /   253 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 72%|███████▏  | 72/100 [05:36<02:02,  4.36s/it]Llama.generate: 570 prefix-match hit, remaining 148 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2084.96 ms /   148 tokens (   14.09 ms per token,    70.98 tokens per second)\n",
            "llama_perf_context_print:        eval time =     205.45 ms /     1 runs   (  205.45 ms per token,     4.87 tokens per second)\n",
            "llama_perf_context_print:       total time =    2296.67 ms /   149 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 73%|███████▎  | 73/100 [05:38<01:41,  3.74s/it]Llama.generate: 570 prefix-match hit, remaining 428 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    5963.66 ms /   428 tokens (   13.93 ms per token,    71.77 tokens per second)\n",
            "llama_perf_context_print:        eval time =     207.79 ms /     1 runs   (  207.79 ms per token,     4.81 tokens per second)\n",
            "llama_perf_context_print:       total time =    6177.55 ms /   429 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 74%|███████▍  | 74/100 [05:44<01:56,  4.48s/it]Llama.generate: 570 prefix-match hit, remaining 124 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3233.96 ms /   124 tokens (   26.08 ms per token,    38.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =     203.11 ms /     1 runs   (  203.11 ms per token,     4.92 tokens per second)\n",
            "llama_perf_context_print:       total time =    3442.76 ms /   125 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 75%|███████▌  | 75/100 [05:48<01:44,  4.17s/it]Llama.generate: 570 prefix-match hit, remaining 454 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    6331.01 ms /   454 tokens (   13.94 ms per token,    71.71 tokens per second)\n",
            "llama_perf_context_print:        eval time =     207.39 ms /     1 runs   (  207.39 ms per token,     4.82 tokens per second)\n",
            "llama_perf_context_print:       total time =    6544.52 ms /   455 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 76%|███████▌  | 76/100 [05:54<01:57,  4.88s/it]Llama.generate: 570 prefix-match hit, remaining 228 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3354.19 ms /   228 tokens (   14.71 ms per token,    67.97 tokens per second)\n",
            "llama_perf_context_print:        eval time =     209.92 ms /     1 runs   (  209.92 ms per token,     4.76 tokens per second)\n",
            "llama_perf_context_print:       total time =    3570.13 ms /   229 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 77%|███████▋  | 77/100 [05:58<01:43,  4.49s/it]Llama.generate: 570 prefix-match hit, remaining 230 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3989.09 ms /   230 tokens (   17.34 ms per token,    57.66 tokens per second)\n",
            "llama_perf_context_print:        eval time =     205.59 ms /     1 runs   (  205.59 ms per token,     4.86 tokens per second)\n",
            "llama_perf_context_print:       total time =    4200.57 ms /   231 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 78%|███████▊  | 78/100 [06:02<01:36,  4.41s/it]Llama.generate: 570 prefix-match hit, remaining 102 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    1485.92 ms /   102 tokens (   14.57 ms per token,    68.64 tokens per second)\n",
            "llama_perf_context_print:        eval time =     203.50 ms /     1 runs   (  203.50 ms per token,     4.91 tokens per second)\n",
            "llama_perf_context_print:       total time =    1695.06 ms /   103 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 79%|███████▉  | 79/100 [06:04<01:15,  3.60s/it]Llama.generate: 570 prefix-match hit, remaining 180 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2546.70 ms /   180 tokens (   14.15 ms per token,    70.68 tokens per second)\n",
            "llama_perf_context_print:        eval time =     204.70 ms /     1 runs   (  204.70 ms per token,     4.89 tokens per second)\n",
            "llama_perf_context_print:       total time =    2757.11 ms /   181 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 80%|████████  | 80/100 [06:07<01:06,  3.35s/it]Llama.generate: 570 prefix-match hit, remaining 166 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2351.17 ms /   166 tokens (   14.16 ms per token,    70.60 tokens per second)\n",
            "llama_perf_context_print:        eval time =     204.01 ms /     1 runs   (  204.01 ms per token,     4.90 tokens per second)\n",
            "llama_perf_context_print:       total time =    2560.97 ms /   167 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 81%|████████  | 81/100 [06:09<00:59,  3.11s/it]Llama.generate: 570 prefix-match hit, remaining 271 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    5054.61 ms /   271 tokens (   18.65 ms per token,    53.61 tokens per second)\n",
            "llama_perf_context_print:        eval time =     207.07 ms /     1 runs   (  207.07 ms per token,     4.83 tokens per second)\n",
            "llama_perf_context_print:       total time =    5267.90 ms /   272 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 82%|████████▏ | 82/100 [06:15<01:07,  3.76s/it]Llama.generate: 570 prefix-match hit, remaining 306 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    4289.53 ms /   306 tokens (   14.02 ms per token,    71.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =     208.58 ms /     1 runs   (  208.58 ms per token,     4.79 tokens per second)\n",
            "llama_perf_context_print:       total time =    4504.02 ms /   307 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 83%|████████▎ | 83/100 [06:19<01:07,  3.99s/it]Llama.generate: 570 prefix-match hit, remaining 164 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2299.18 ms /   164 tokens (   14.02 ms per token,    71.33 tokens per second)\n",
            "llama_perf_context_print:        eval time =     208.05 ms /     1 runs   (  208.05 ms per token,     4.81 tokens per second)\n",
            "llama_perf_context_print:       total time =    2513.01 ms /   165 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 84%|████████▍ | 84/100 [06:22<00:56,  3.55s/it]Llama.generate: 570 prefix-match hit, remaining 421 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    6939.46 ms /   421 tokens (   16.48 ms per token,    60.67 tokens per second)\n",
            "llama_perf_context_print:        eval time =     209.89 ms /     1 runs   (  209.89 ms per token,     4.76 tokens per second)\n",
            "llama_perf_context_print:       total time =    7155.94 ms /   422 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 85%|████████▌ | 85/100 [06:29<01:09,  4.63s/it]Llama.generate: 570 prefix-match hit, remaining 226 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3156.24 ms /   226 tokens (   13.97 ms per token,    71.60 tokens per second)\n",
            "llama_perf_context_print:        eval time =     205.74 ms /     1 runs   (  205.74 ms per token,     4.86 tokens per second)\n",
            "llama_perf_context_print:       total time =    3367.86 ms /   227 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 86%|████████▌ | 86/100 [06:32<00:59,  4.26s/it]Llama.generate: 570 prefix-match hit, remaining 213 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3652.38 ms /   213 tokens (   17.15 ms per token,    58.32 tokens per second)\n",
            "llama_perf_context_print:        eval time =     206.74 ms /     1 runs   (  206.74 ms per token,     4.84 tokens per second)\n",
            "llama_perf_context_print:       total time =    3865.59 ms /   214 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 87%|████████▋ | 87/100 [06:36<00:53,  4.14s/it]Llama.generate: 570 prefix-match hit, remaining 402 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    6015.19 ms /   402 tokens (   14.96 ms per token,    66.83 tokens per second)\n",
            "llama_perf_context_print:        eval time =     209.04 ms /     1 runs   (  209.04 ms per token,     4.78 tokens per second)\n",
            "llama_perf_context_print:       total time =    6230.38 ms /   403 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 88%|████████▊ | 88/100 [06:42<00:57,  4.77s/it]Llama.generate: 570 prefix-match hit, remaining 173 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2475.45 ms /   173 tokens (   14.31 ms per token,    69.89 tokens per second)\n",
            "llama_perf_context_print:        eval time =     204.47 ms /     1 runs   (  204.47 ms per token,     4.89 tokens per second)\n",
            "llama_perf_context_print:       total time =    2686.14 ms /   174 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 89%|████████▉ | 89/100 [06:45<00:45,  4.15s/it]Llama.generate: 570 prefix-match hit, remaining 229 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    4407.09 ms /   229 tokens (   19.24 ms per token,    51.96 tokens per second)\n",
            "llama_perf_context_print:        eval time =     218.47 ms /     1 runs   (  218.47 ms per token,     4.58 tokens per second)\n",
            "llama_perf_context_print:       total time =    4631.61 ms /   230 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 90%|█████████ | 90/100 [06:50<00:42,  4.30s/it]Llama.generate: 570 prefix-match hit, remaining 155 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2226.93 ms /   155 tokens (   14.37 ms per token,    69.60 tokens per second)\n",
            "llama_perf_context_print:        eval time =     205.55 ms /     1 runs   (  205.55 ms per token,     4.86 tokens per second)\n",
            "llama_perf_context_print:       total time =    2438.20 ms /   156 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 91%|█████████ | 91/100 [06:52<00:33,  3.74s/it]Llama.generate: 570 prefix-match hit, remaining 260 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3620.12 ms /   260 tokens (   13.92 ms per token,    71.82 tokens per second)\n",
            "llama_perf_context_print:        eval time =     206.32 ms /     1 runs   (  206.32 ms per token,     4.85 tokens per second)\n",
            "llama_perf_context_print:       total time =    3832.37 ms /   261 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 92%|█████████▏| 92/100 [06:56<00:30,  3.77s/it]Llama.generate: 570 prefix-match hit, remaining 302 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    5137.05 ms /   302 tokens (   17.01 ms per token,    58.79 tokens per second)\n",
            "llama_perf_context_print:        eval time =     205.49 ms /     1 runs   (  205.49 ms per token,     4.87 tokens per second)\n",
            "llama_perf_context_print:       total time =    5348.43 ms /   303 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 93%|█████████▎| 93/100 [07:01<00:29,  4.25s/it]Llama.generate: 570 prefix-match hit, remaining 272 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3952.04 ms /   272 tokens (   14.53 ms per token,    68.83 tokens per second)\n",
            "llama_perf_context_print:        eval time =     207.83 ms /     1 runs   (  207.83 ms per token,     4.81 tokens per second)\n",
            "llama_perf_context_print:       total time =    4165.93 ms /   273 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 94%|█████████▍| 94/100 [07:05<00:25,  4.22s/it]Llama.generate: 570 prefix-match hit, remaining 353 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    4922.40 ms /   353 tokens (   13.94 ms per token,    71.71 tokens per second)\n",
            "llama_perf_context_print:        eval time =     209.29 ms /     1 runs   (  209.29 ms per token,     4.78 tokens per second)\n",
            "llama_perf_context_print:       total time =    5137.69 ms /   354 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 95%|█████████▌| 95/100 [07:11<00:22,  4.50s/it]Llama.generate: 570 prefix-match hit, remaining 310 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    5523.38 ms /   310 tokens (   17.82 ms per token,    56.13 tokens per second)\n",
            "llama_perf_context_print:        eval time =     204.13 ms /     1 runs   (  204.13 ms per token,     4.90 tokens per second)\n",
            "llama_perf_context_print:       total time =    5733.49 ms /   311 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 96%|█████████▌| 96/100 [07:16<00:19,  4.87s/it]Llama.generate: 570 prefix-match hit, remaining 171 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2465.84 ms /   171 tokens (   14.42 ms per token,    69.35 tokens per second)\n",
            "llama_perf_context_print:        eval time =     202.60 ms /     1 runs   (  202.60 ms per token,     4.94 tokens per second)\n",
            "llama_perf_context_print:       total time =    2674.39 ms /   172 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 97%|█████████▋| 97/100 [07:19<00:12,  4.22s/it]Llama.generate: 570 prefix-match hit, remaining 165 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2339.74 ms /   165 tokens (   14.18 ms per token,    70.52 tokens per second)\n",
            "llama_perf_context_print:        eval time =     206.46 ms /     1 runs   (  206.46 ms per token,     4.84 tokens per second)\n",
            "llama_perf_context_print:       total time =    2552.00 ms /   166 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 98%|█████████▊| 98/100 [07:22<00:07,  3.72s/it]Llama.generate: 570 prefix-match hit, remaining 169 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2406.00 ms /   169 tokens (   14.24 ms per token,    70.24 tokens per second)\n",
            "llama_perf_context_print:        eval time =     203.44 ms /     1 runs   (  203.44 ms per token,     4.92 tokens per second)\n",
            "llama_perf_context_print:       total time =    2615.21 ms /   170 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            " 99%|█████████▉| 99/100 [07:24<00:03,  3.39s/it]Llama.generate: 570 prefix-match hit, remaining 183 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3950.65 ms /   183 tokens (   21.59 ms per token,    46.32 tokens per second)\n",
            "llama_perf_context_print:        eval time =     204.04 ms /     1 runs   (  204.04 ms per token,     4.90 tokens per second)\n",
            "llama_perf_context_print:       total time =    4160.46 ms /   184 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "100%|██████████| 100/100 [07:28<00:00,  4.49s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for few_shot: 0.84\n",
            "Running evaluation for strategy: cot...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/100 [00:00<?, ?it/s]Llama.generate: 32 prefix-match hit, remaining 275 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3658.75 ms /   275 tokens (   13.30 ms per token,    75.16 tokens per second)\n",
            "llama_perf_context_print:        eval time =     393.57 ms /     2 runs   (  196.78 ms per token,     5.08 tokens per second)\n",
            "llama_perf_context_print:       total time =    4060.98 ms /   277 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            "  1%|          | 1/100 [00:04<06:42,  4.07s/it]Llama.generate: 35 prefix-match hit, remaining 411 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    6420.25 ms /   411 tokens (   15.62 ms per token,    64.02 tokens per second)\n",
            "llama_perf_context_print:        eval time =   12746.67 ms /    64 runs   (  199.17 ms per token,     5.02 tokens per second)\n",
            "llama_perf_context_print:       total time =   19333.71 ms /   475 tokens\n",
            "llama_perf_context_print:    graphs reused =         61\n",
            "  2%|▏         | 2/100 [00:23<21:18, 13.05s/it]Llama.generate: 35 prefix-match hit, remaining 705 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =   10518.39 ms /   705 tokens (   14.92 ms per token,    67.03 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1625.08 ms /     8 runs   (  203.14 ms per token,     4.92 tokens per second)\n",
            "llama_perf_context_print:       total time =   12166.96 ms /   713 tokens\n",
            "llama_perf_context_print:    graphs reused =          7\n",
            "  3%|▎         | 3/100 [00:35<20:27, 12.65s/it]Llama.generate: 35 prefix-match hit, remaining 378 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    5071.48 ms /   378 tokens (   13.42 ms per token,    74.53 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1591.59 ms /     8 runs   (  198.95 ms per token,     5.03 tokens per second)\n",
            "llama_perf_context_print:       total time =    6686.72 ms /   386 tokens\n",
            "llama_perf_context_print:    graphs reused =          6\n",
            "  4%|▍         | 4/100 [00:42<16:28, 10.30s/it]Llama.generate: 35 prefix-match hit, remaining 129 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    1722.24 ms /   129 tokens (   13.35 ms per token,    74.90 tokens per second)\n",
            "llama_perf_context_print:        eval time =   15120.02 ms /    77 runs   (  196.36 ms per token,     5.09 tokens per second)\n",
            "llama_perf_context_print:       total time =   17041.78 ms /   206 tokens\n",
            "llama_perf_context_print:    graphs reused =         74\n",
            "  5%|▌         | 5/100 [00:59<20:09, 12.73s/it]Llama.generate: 35 prefix-match hit, remaining 213 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2866.26 ms /   213 tokens (   13.46 ms per token,    74.31 tokens per second)\n",
            "llama_perf_context_print:        eval time =   14477.54 ms /    74 runs   (  195.64 ms per token,     5.11 tokens per second)\n",
            "llama_perf_context_print:       total time =   17535.52 ms /   287 tokens\n",
            "llama_perf_context_print:    graphs reused =         70\n",
            "  6%|▌         | 6/100 [01:16<22:30, 14.37s/it]Llama.generate: 35 prefix-match hit, remaining 155 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2990.34 ms /   155 tokens (   19.29 ms per token,    51.83 tokens per second)\n",
            "llama_perf_context_print:        eval time =   12196.07 ms /    62 runs   (  196.71 ms per token,     5.08 tokens per second)\n",
            "llama_perf_context_print:       total time =   15347.10 ms /   217 tokens\n",
            "llama_perf_context_print:    graphs reused =         59\n",
            "  7%|▋         | 7/100 [01:32<22:46, 14.69s/it]Llama.generate: 35 prefix-match hit, remaining 192 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2558.98 ms /   192 tokens (   13.33 ms per token,    75.03 tokens per second)\n",
            "llama_perf_context_print:        eval time =   10906.23 ms /    56 runs   (  194.75 ms per token,     5.13 tokens per second)\n",
            "llama_perf_context_print:       total time =   13610.41 ms /   248 tokens\n",
            "llama_perf_context_print:    graphs reused =         54\n",
            "  8%|▊         | 8/100 [01:45<21:59, 14.35s/it]Llama.generate: 35 prefix-match hit, remaining 317 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    4251.14 ms /   317 tokens (   13.41 ms per token,    74.57 tokens per second)\n",
            "llama_perf_context_print:        eval time =   18461.39 ms /    93 runs   (  198.51 ms per token,     5.04 tokens per second)\n",
            "llama_perf_context_print:       total time =   22955.21 ms /   410 tokens\n",
            "llama_perf_context_print:    graphs reused =         90\n",
            "  9%|▉         | 9/100 [02:08<25:50, 17.04s/it]Llama.generate: 35 prefix-match hit, remaining 562 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    8720.21 ms /   562 tokens (   15.52 ms per token,    64.45 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1624.09 ms /     8 runs   (  203.01 ms per token,     4.93 tokens per second)\n",
            "llama_perf_context_print:       total time =   10367.92 ms /   570 tokens\n",
            "llama_perf_context_print:    graphs reused =          7\n",
            " 10%|█         | 10/100 [02:19<22:28, 14.98s/it]Llama.generate: 35 prefix-match hit, remaining 653 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    8820.56 ms /   653 tokens (   13.51 ms per token,    74.03 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1610.62 ms /     8 runs   (  201.33 ms per token,     4.97 tokens per second)\n",
            "llama_perf_context_print:       total time =   10454.83 ms /   661 tokens\n",
            "llama_perf_context_print:    graphs reused =          7\n",
            " 11%|█         | 11/100 [02:29<20:10, 13.60s/it]Llama.generate: 36 prefix-match hit, remaining 190 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2582.38 ms /   190 tokens (   13.59 ms per token,    73.58 tokens per second)\n",
            "llama_perf_context_print:        eval time =   19518.78 ms /    99 runs   (  197.16 ms per token,     5.07 tokens per second)\n",
            "llama_perf_context_print:       total time =   22360.57 ms /   289 tokens\n",
            "llama_perf_context_print:    graphs reused =         95\n",
            " 12%|█▏        | 12/100 [02:51<23:51, 16.27s/it]Llama.generate: 35 prefix-match hit, remaining 273 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3772.53 ms /   273 tokens (   13.82 ms per token,    72.37 tokens per second)\n",
            "llama_perf_context_print:        eval time =   17376.73 ms /    87 runs   (  199.73 ms per token,     5.01 tokens per second)\n",
            "llama_perf_context_print:       total time =   21378.17 ms /   360 tokens\n",
            "llama_perf_context_print:    graphs reused =         83\n",
            " 13%|█▎        | 13/100 [03:13<25:50, 17.82s/it]Llama.generate: 35 prefix-match hit, remaining 273 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    4854.50 ms /   273 tokens (   17.78 ms per token,    56.24 tokens per second)\n",
            "llama_perf_context_print:        eval time =   12748.37 ms /    64 runs   (  199.19 ms per token,     5.02 tokens per second)\n",
            "llama_perf_context_print:       total time =   17768.37 ms /   337 tokens\n",
            "llama_perf_context_print:    graphs reused =         61\n",
            " 14%|█▍        | 14/100 [03:31<25:31, 17.80s/it]Llama.generate: 35 prefix-match hit, remaining 300 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3960.75 ms /   300 tokens (   13.20 ms per token,    75.74 tokens per second)\n",
            "llama_perf_context_print:        eval time =   15546.76 ms /    78 runs   (  199.32 ms per token,     5.02 tokens per second)\n",
            "llama_perf_context_print:       total time =   19710.47 ms /   378 tokens\n",
            "llama_perf_context_print:    graphs reused =         75\n",
            " 15%|█▌        | 15/100 [03:50<26:02, 18.38s/it]Llama.generate: 35 prefix-match hit, remaining 193 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2549.00 ms /   193 tokens (   13.21 ms per token,    75.72 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1569.73 ms /     8 runs   (  196.22 ms per token,     5.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    4142.06 ms /   201 tokens\n",
            "llama_perf_context_print:    graphs reused =          7\n",
            " 16%|█▌        | 16/100 [03:55<19:44, 14.10s/it]Llama.generate: 35 prefix-match hit, remaining 291 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3861.73 ms /   291 tokens (   13.27 ms per token,    75.35 tokens per second)\n",
            "llama_perf_context_print:        eval time =    8543.70 ms /    43 runs   (  198.69 ms per token,     5.03 tokens per second)\n",
            "llama_perf_context_print:       total time =   12516.97 ms /   334 tokens\n",
            "llama_perf_context_print:    graphs reused =         41\n",
            " 17%|█▋        | 17/100 [04:07<18:50, 13.62s/it]Llama.generate: 35 prefix-match hit, remaining 289 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3852.90 ms /   289 tokens (   13.33 ms per token,    75.01 tokens per second)\n",
            "llama_perf_context_print:        eval time =   19697.21 ms /    99 runs   (  198.96 ms per token,     5.03 tokens per second)\n",
            "llama_perf_context_print:       total time =   23808.32 ms /   388 tokens\n",
            "llama_perf_context_print:    graphs reused =         95\n",
            " 18%|█▊        | 18/100 [04:31<22:48, 16.68s/it]Llama.generate: 35 prefix-match hit, remaining 213 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2841.57 ms /   213 tokens (   13.34 ms per token,    74.96 tokens per second)\n",
            "llama_perf_context_print:        eval time =   12807.54 ms /    65 runs   (  197.04 ms per token,     5.08 tokens per second)\n",
            "llama_perf_context_print:       total time =   15818.57 ms /   278 tokens\n",
            "llama_perf_context_print:    graphs reused =         62\n",
            " 19%|█▉        | 19/100 [04:47<22:10, 16.43s/it]Llama.generate: 35 prefix-match hit, remaining 235 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3279.16 ms /   235 tokens (   13.95 ms per token,    71.66 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1599.31 ms /     8 runs   (  199.91 ms per token,     5.00 tokens per second)\n",
            "llama_perf_context_print:       total time =    4901.56 ms /   243 tokens\n",
            "llama_perf_context_print:    graphs reused =          7\n",
            " 20%|██        | 20/100 [04:52<17:17, 12.97s/it]Llama.generate: 35 prefix-match hit, remaining 113 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    1551.68 ms /   113 tokens (   13.73 ms per token,    72.82 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1592.82 ms /     8 runs   (  199.10 ms per token,     5.02 tokens per second)\n",
            "llama_perf_context_print:       total time =    3167.51 ms /   121 tokens\n",
            "llama_perf_context_print:    graphs reused =          7\n",
            " 21%|██        | 21/100 [04:55<13:12, 10.03s/it]Llama.generate: 35 prefix-match hit, remaining 396 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    6373.02 ms /   396 tokens (   16.09 ms per token,    62.14 tokens per second)\n",
            "llama_perf_context_print:        eval time =   14334.46 ms /    72 runs   (  199.09 ms per token,     5.02 tokens per second)\n",
            "llama_perf_context_print:       total time =   20894.92 ms /   468 tokens\n",
            "llama_perf_context_print:    graphs reused =         69\n",
            " 22%|██▏       | 22/100 [05:16<17:16, 13.29s/it]Llama.generate: 35 prefix-match hit, remaining 215 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2890.79 ms /   215 tokens (   13.45 ms per token,    74.37 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1573.78 ms /     8 runs   (  196.72 ms per token,     5.08 tokens per second)\n",
            "llama_perf_context_print:       total time =    4487.75 ms /   223 tokens\n",
            "llama_perf_context_print:    graphs reused =          6\n",
            " 23%|██▎       | 23/100 [05:20<13:40, 10.65s/it]Llama.generate: 35 prefix-match hit, remaining 171 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3632.74 ms /   171 tokens (   21.24 ms per token,    47.07 tokens per second)\n",
            "llama_perf_context_print:        eval time =   16324.54 ms /    83 runs   (  196.68 ms per token,     5.08 tokens per second)\n",
            "llama_perf_context_print:       total time =   20173.59 ms /   254 tokens\n",
            "llama_perf_context_print:    graphs reused =         79\n",
            " 24%|██▍       | 24/100 [05:40<17:06, 13.51s/it]Llama.generate: 35 prefix-match hit, remaining 282 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3791.02 ms /   282 tokens (   13.44 ms per token,    74.39 tokens per second)\n",
            "llama_perf_context_print:        eval time =   11063.06 ms /    56 runs   (  197.55 ms per token,     5.06 tokens per second)\n",
            "llama_perf_context_print:       total time =   14999.06 ms /   338 tokens\n",
            "llama_perf_context_print:    graphs reused =         53\n",
            " 25%|██▌       | 25/100 [05:55<17:26, 13.96s/it]Llama.generate: 35 prefix-match hit, remaining 190 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3949.17 ms /   190 tokens (   20.79 ms per token,    48.11 tokens per second)\n",
            "llama_perf_context_print:        eval time =   13392.73 ms /    68 runs   (  196.95 ms per token,     5.08 tokens per second)\n",
            "llama_perf_context_print:       total time =   17519.56 ms /   258 tokens\n",
            "llama_perf_context_print:    graphs reused =         65\n",
            " 26%|██▌       | 26/100 [06:13<18:32, 15.03s/it]Llama.generate: 35 prefix-match hit, remaining 176 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2353.22 ms /   176 tokens (   13.37 ms per token,    74.79 tokens per second)\n",
            "llama_perf_context_print:        eval time =   12489.10 ms /    64 runs   (  195.14 ms per token,     5.12 tokens per second)\n",
            "llama_perf_context_print:       total time =   15008.26 ms /   240 tokens\n",
            "llama_perf_context_print:    graphs reused =         61\n",
            " 27%|██▋       | 27/100 [06:28<18:16, 15.02s/it]Llama.generate: 35 prefix-match hit, remaining 402 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    6439.89 ms /   402 tokens (   16.02 ms per token,    62.42 tokens per second)\n",
            "llama_perf_context_print:        eval time =   12347.35 ms /    62 runs   (  199.15 ms per token,     5.02 tokens per second)\n",
            "llama_perf_context_print:       total time =   18948.92 ms /   464 tokens\n",
            "llama_perf_context_print:    graphs reused =         59\n",
            " 28%|██▊       | 28/100 [06:47<19:26, 16.20s/it]Llama.generate: 35 prefix-match hit, remaining 245 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3269.47 ms /   245 tokens (   13.34 ms per token,    74.94 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1594.87 ms /     8 runs   (  199.36 ms per token,     5.02 tokens per second)\n",
            "llama_perf_context_print:       total time =    4887.66 ms /   253 tokens\n",
            "llama_perf_context_print:    graphs reused =          7\n",
            " 29%|██▉       | 29/100 [06:52<15:09, 12.81s/it]Llama.generate: 35 prefix-match hit, remaining 495 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    7695.30 ms /   495 tokens (   15.55 ms per token,    64.32 tokens per second)\n",
            "llama_perf_context_print:        eval time =   12223.30 ms /    61 runs   (  200.38 ms per token,     4.99 tokens per second)\n",
            "llama_perf_context_print:       total time =   20077.69 ms /   556 tokens\n",
            "llama_perf_context_print:    graphs reused =         58\n",
            " 30%|███       | 30/100 [07:12<17:29, 14.99s/it]Llama.generate: 35 prefix-match hit, remaining 446 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    6029.11 ms /   446 tokens (   13.52 ms per token,    73.97 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1595.45 ms /     8 runs   (  199.43 ms per token,     5.01 tokens per second)\n",
            "llama_perf_context_print:       total time =    7648.34 ms /   454 tokens\n",
            "llama_perf_context_print:    graphs reused =          7\n",
            " 31%|███       | 31/100 [07:19<14:42, 12.79s/it]Llama.generate: 35 prefix-match hit, remaining 188 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3595.80 ms /   188 tokens (   19.13 ms per token,    52.28 tokens per second)\n",
            "llama_perf_context_print:        eval time =   14154.77 ms /    72 runs   (  196.59 ms per token,     5.09 tokens per second)\n",
            "llama_perf_context_print:       total time =   17937.24 ms /   260 tokens\n",
            "llama_perf_context_print:    graphs reused =         68\n",
            " 32%|███▏      | 32/100 [07:37<16:14, 14.34s/it]Llama.generate: 35 prefix-match hit, remaining 447 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    6314.14 ms /   447 tokens (   14.13 ms per token,    70.79 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1601.07 ms /     8 runs   (  200.13 ms per token,     5.00 tokens per second)\n",
            "llama_perf_context_print:       total time =    7938.64 ms /   455 tokens\n",
            "llama_perf_context_print:    graphs reused =          7\n",
            " 33%|███▎      | 33/100 [07:45<13:52, 12.42s/it]Llama.generate: 35 prefix-match hit, remaining 187 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2589.10 ms /   187 tokens (   13.85 ms per token,    72.23 tokens per second)\n",
            "llama_perf_context_print:        eval time =    9628.18 ms /    49 runs   (  196.49 ms per token,     5.09 tokens per second)\n",
            "llama_perf_context_print:       total time =   12344.35 ms /   236 tokens\n",
            "llama_perf_context_print:    graphs reused =         46\n",
            " 34%|███▍      | 34/100 [07:58<13:38, 12.40s/it]Llama.generate: 35 prefix-match hit, remaining 177 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2386.32 ms /   177 tokens (   13.48 ms per token,    74.17 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1575.93 ms /     8 runs   (  196.99 ms per token,     5.08 tokens per second)\n",
            "llama_perf_context_print:       total time =    3985.13 ms /   185 tokens\n",
            "llama_perf_context_print:    graphs reused =          7\n",
            " 35%|███▌      | 35/100 [08:02<10:41,  9.88s/it]Llama.generate: 36 prefix-match hit, remaining 99 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    1383.91 ms /    99 tokens (   13.98 ms per token,    71.54 tokens per second)\n",
            "llama_perf_context_print:        eval time =    9404.95 ms /    48 runs   (  195.94 ms per token,     5.10 tokens per second)\n",
            "llama_perf_context_print:       total time =   10913.58 ms /   147 tokens\n",
            "llama_perf_context_print:    graphs reused =         46\n",
            " 36%|███▌      | 36/100 [08:13<10:52, 10.19s/it]Llama.generate: 35 prefix-match hit, remaining 323 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    4323.78 ms /   323 tokens (   13.39 ms per token,    74.70 tokens per second)\n",
            "llama_perf_context_print:        eval time =   17153.92 ms /    86 runs   (  199.46 ms per token,     5.01 tokens per second)\n",
            "llama_perf_context_print:       total time =   21702.93 ms /   409 tokens\n",
            "llama_perf_context_print:    graphs reused =         83\n",
            " 37%|███▋      | 37/100 [08:34<14:19, 13.64s/it]Llama.generate: 35 prefix-match hit, remaining 421 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    5600.86 ms /   421 tokens (   13.30 ms per token,    75.17 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1598.05 ms /     8 runs   (  199.76 ms per token,     5.01 tokens per second)\n",
            "llama_perf_context_print:       total time =    7222.48 ms /   429 tokens\n",
            "llama_perf_context_print:    graphs reused =          7\n",
            " 38%|███▊      | 38/100 [08:42<12:06, 11.72s/it]Llama.generate: 35 prefix-match hit, remaining 482 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    6522.94 ms /   482 tokens (   13.53 ms per token,    73.89 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1610.44 ms /     8 runs   (  201.30 ms per token,     4.97 tokens per second)\n",
            "llama_perf_context_print:       total time =    8157.05 ms /   490 tokens\n",
            "llama_perf_context_print:    graphs reused =          7\n",
            " 39%|███▉      | 39/100 [08:50<10:49, 10.65s/it]Llama.generate: 35 prefix-match hit, remaining 442 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    6958.17 ms /   442 tokens (   15.74 ms per token,    63.52 tokens per second)\n",
            "llama_perf_context_print:        eval time =   16817.26 ms /    84 runs   (  200.21 ms per token,     4.99 tokens per second)\n",
            "llama_perf_context_print:       total time =   23995.68 ms /   526 tokens\n",
            "llama_perf_context_print:    graphs reused =         80\n",
            " 40%|████      | 40/100 [09:14<14:39, 14.66s/it]Llama.generate: 35 prefix-match hit, remaining 189 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3341.38 ms /   189 tokens (   17.68 ms per token,    56.56 tokens per second)\n",
            "llama_perf_context_print:        eval time =   17351.62 ms /    89 runs   (  194.96 ms per token,     5.13 tokens per second)\n",
            "llama_perf_context_print:       total time =   20924.36 ms /   278 tokens\n",
            "llama_perf_context_print:    graphs reused =         86\n",
            " 41%|████      | 41/100 [09:35<16:15, 16.54s/it]Llama.generate: 35 prefix-match hit, remaining 804 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =   11963.44 ms /   804 tokens (   14.88 ms per token,    67.20 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1646.79 ms /     8 runs   (  205.85 ms per token,     4.86 tokens per second)\n",
            "llama_perf_context_print:       total time =   13633.72 ms /   812 tokens\n",
            "llama_perf_context_print:    graphs reused =          7\n",
            " 42%|████▏     | 42/100 [09:48<15:08, 15.67s/it]Llama.generate: 36 prefix-match hit, remaining 321 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    5254.89 ms /   321 tokens (   16.37 ms per token,    61.09 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1589.29 ms /     8 runs   (  198.66 ms per token,     5.03 tokens per second)\n",
            "llama_perf_context_print:       total time =    6868.07 ms /   329 tokens\n",
            "llama_perf_context_print:    graphs reused =          7\n",
            " 43%|████▎     | 43/100 [09:55<12:22, 13.03s/it]Llama.generate: 36 prefix-match hit, remaining 156 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2112.62 ms /   156 tokens (   13.54 ms per token,    73.84 tokens per second)\n",
            "llama_perf_context_print:        eval time =   11021.79 ms /    56 runs   (  196.82 ms per token,     5.08 tokens per second)\n",
            "llama_perf_context_print:       total time =   13279.71 ms /   212 tokens\n",
            "llama_perf_context_print:    graphs reused =         54\n",
            " 44%|████▍     | 44/100 [10:08<12:14, 13.11s/it]Llama.generate: 35 prefix-match hit, remaining 209 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2820.41 ms /   209 tokens (   13.49 ms per token,    74.10 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1562.67 ms /     8 runs   (  195.33 ms per token,     5.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    4406.22 ms /   217 tokens\n",
            "llama_perf_context_print:    graphs reused =          7\n",
            " 45%|████▌     | 45/100 [10:13<09:37, 10.50s/it]Llama.generate: 35 prefix-match hit, remaining 176 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2343.71 ms /   176 tokens (   13.32 ms per token,    75.09 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1594.64 ms /     8 runs   (  199.33 ms per token,     5.02 tokens per second)\n",
            "llama_perf_context_print:       total time =    3961.76 ms /   184 tokens\n",
            "llama_perf_context_print:    graphs reused =          7\n",
            " 46%|████▌     | 46/100 [10:17<07:41,  8.54s/it]Llama.generate: 35 prefix-match hit, remaining 471 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    6336.30 ms /   471 tokens (   13.45 ms per token,    74.33 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1611.36 ms /     8 runs   (  201.42 ms per token,     4.96 tokens per second)\n",
            "llama_perf_context_print:       total time =    7971.36 ms /   479 tokens\n",
            "llama_perf_context_print:    graphs reused =          6\n",
            " 47%|████▋     | 47/100 [10:25<07:23,  8.37s/it]Llama.generate: 35 prefix-match hit, remaining 165 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2738.41 ms /   165 tokens (   16.60 ms per token,    60.25 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1578.96 ms /     8 runs   (  197.37 ms per token,     5.07 tokens per second)\n",
            "llama_perf_context_print:       total time =    4340.47 ms /   173 tokens\n",
            "llama_perf_context_print:    graphs reused =          7\n",
            " 48%|████▊     | 48/100 [10:29<06:12,  7.16s/it]Llama.generate: 35 prefix-match hit, remaining 290 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3909.58 ms /   290 tokens (   13.48 ms per token,    74.18 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1601.42 ms /     8 runs   (  200.18 ms per token,     5.00 tokens per second)\n",
            "llama_perf_context_print:       total time =    5534.18 ms /   298 tokens\n",
            "llama_perf_context_print:    graphs reused =          7\n",
            " 49%|████▉     | 49/100 [10:35<05:40,  6.68s/it]Llama.generate: 35 prefix-match hit, remaining 263 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3575.23 ms /   263 tokens (   13.59 ms per token,    73.56 tokens per second)\n",
            "llama_perf_context_print:        eval time =   14982.53 ms /    76 runs   (  197.14 ms per token,     5.07 tokens per second)\n",
            "llama_perf_context_print:       total time =   18755.69 ms /   339 tokens\n",
            "llama_perf_context_print:    graphs reused =         73\n",
            " 50%|█████     | 50/100 [10:53<08:35, 10.30s/it]Llama.generate: 35 prefix-match hit, remaining 149 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2060.91 ms /   149 tokens (   13.83 ms per token,    72.30 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1567.81 ms /     8 runs   (  195.98 ms per token,     5.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    3651.93 ms /   157 tokens\n",
            "llama_perf_context_print:    graphs reused =          7\n",
            " 51%|█████     | 51/100 [10:57<06:47,  8.31s/it]Llama.generate: 35 prefix-match hit, remaining 176 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2341.89 ms /   176 tokens (   13.31 ms per token,    75.15 tokens per second)\n",
            "llama_perf_context_print:        eval time =   11367.44 ms /    58 runs   (  195.99 ms per token,     5.10 tokens per second)\n",
            "llama_perf_context_print:       total time =   13861.24 ms /   234 tokens\n",
            "llama_perf_context_print:    graphs reused =         55\n",
            " 52%|█████▏    | 52/100 [11:11<07:58,  9.98s/it]Llama.generate: 35 prefix-match hit, remaining 90 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    1315.46 ms /    90 tokens (   14.62 ms per token,    68.42 tokens per second)\n",
            "llama_perf_context_print:        eval time =   10230.12 ms /    52 runs   (  196.73 ms per token,     5.08 tokens per second)\n",
            "llama_perf_context_print:       total time =   11680.35 ms /   142 tokens\n",
            "llama_perf_context_print:    graphs reused =         49\n",
            " 53%|█████▎    | 53/100 [11:23<08:12, 10.49s/it]Llama.generate: 35 prefix-match hit, remaining 167 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3427.44 ms /   167 tokens (   20.52 ms per token,    48.72 tokens per second)\n",
            "llama_perf_context_print:        eval time =   16993.25 ms /    86 runs   (  197.60 ms per token,     5.06 tokens per second)\n",
            "llama_perf_context_print:       total time =   20644.01 ms /   253 tokens\n",
            "llama_perf_context_print:    graphs reused =         83\n",
            " 54%|█████▍    | 54/100 [11:43<10:22, 13.54s/it]Llama.generate: 35 prefix-match hit, remaining 303 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    4100.24 ms /   303 tokens (   13.53 ms per token,    73.90 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1601.71 ms /     8 runs   (  200.21 ms per token,     4.99 tokens per second)\n",
            "llama_perf_context_print:       total time =    5725.39 ms /   311 tokens\n",
            "llama_perf_context_print:    graphs reused =          7\n",
            " 55%|█████▌    | 55/100 [11:49<08:23, 11.19s/it]Llama.generate: 35 prefix-match hit, remaining 341 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    4610.54 ms /   341 tokens (   13.52 ms per token,    73.96 tokens per second)\n",
            "llama_perf_context_print:        eval time =   13326.10 ms /    67 runs   (  198.90 ms per token,     5.03 tokens per second)\n",
            "llama_perf_context_print:       total time =   18111.38 ms /   408 tokens\n",
            "llama_perf_context_print:    graphs reused =         64\n",
            " 56%|█████▌    | 56/100 [12:07<09:43, 13.27s/it]Llama.generate: 35 prefix-match hit, remaining 196 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2604.03 ms /   196 tokens (   13.29 ms per token,    75.27 tokens per second)\n",
            "llama_perf_context_print:        eval time =    9849.63 ms /    50 runs   (  196.99 ms per token,     5.08 tokens per second)\n",
            "llama_perf_context_print:       total time =   12583.14 ms /   246 tokens\n",
            "llama_perf_context_print:    graphs reused =         48\n",
            " 57%|█████▋    | 57/100 [12:20<09:21, 13.07s/it]Llama.generate: 35 prefix-match hit, remaining 188 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2683.53 ms /   188 tokens (   14.27 ms per token,    70.06 tokens per second)\n",
            "llama_perf_context_print:        eval time =   18130.39 ms /    92 runs   (  197.07 ms per token,     5.07 tokens per second)\n",
            "llama_perf_context_print:       total time =   21053.92 ms /   280 tokens\n",
            "llama_perf_context_print:    graphs reused =         88\n",
            " 58%|█████▊    | 58/100 [12:41<10:49, 15.46s/it]Llama.generate: 35 prefix-match hit, remaining 343 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    4808.11 ms /   343 tokens (   14.02 ms per token,    71.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =   19646.75 ms /    99 runs   (  198.45 ms per token,     5.04 tokens per second)\n",
            "llama_perf_context_print:       total time =   24713.70 ms /   442 tokens\n",
            "llama_perf_context_print:    graphs reused =         95\n",
            " 59%|█████▉    | 59/100 [13:06<12:27, 18.24s/it]Llama.generate: 35 prefix-match hit, remaining 330 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    5499.48 ms /   330 tokens (   16.67 ms per token,    60.01 tokens per second)\n",
            "llama_perf_context_print:        eval time =   15441.07 ms /    78 runs   (  197.96 ms per token,     5.05 tokens per second)\n",
            "llama_perf_context_print:       total time =   21143.03 ms /   408 tokens\n",
            "llama_perf_context_print:    graphs reused =         75\n",
            " 60%|██████    | 60/100 [13:27<12:44, 19.11s/it]Llama.generate: 35 prefix-match hit, remaining 555 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    8572.79 ms /   555 tokens (   15.45 ms per token,    64.74 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1627.47 ms /     8 runs   (  203.43 ms per token,     4.92 tokens per second)\n",
            "llama_perf_context_print:       total time =   10223.80 ms /   563 tokens\n",
            "llama_perf_context_print:    graphs reused =          7\n",
            " 61%|██████    | 61/100 [13:37<10:41, 16.45s/it]Llama.generate: 35 prefix-match hit, remaining 203 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2794.70 ms /   203 tokens (   13.77 ms per token,    72.64 tokens per second)\n",
            "llama_perf_context_print:        eval time =   14178.57 ms /    72 runs   (  196.92 ms per token,     5.08 tokens per second)\n",
            "llama_perf_context_print:       total time =   17159.94 ms /   275 tokens\n",
            "llama_perf_context_print:    graphs reused =         69\n",
            " 62%|██████▏   | 62/100 [13:54<10:33, 16.66s/it]Llama.generate: 35 prefix-match hit, remaining 209 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3803.29 ms /   209 tokens (   18.20 ms per token,    54.95 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1556.88 ms /     8 runs   (  194.61 ms per token,     5.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    5383.57 ms /   217 tokens\n",
            "llama_perf_context_print:    graphs reused =          7\n",
            " 63%|██████▎   | 63/100 [13:59<08:11, 13.28s/it]Llama.generate: 35 prefix-match hit, remaining 397 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    5300.95 ms /   397 tokens (   13.35 ms per token,    74.89 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1585.79 ms /     8 runs   (  198.22 ms per token,     5.04 tokens per second)\n",
            "llama_perf_context_print:       total time =    6910.15 ms /   405 tokens\n",
            "llama_perf_context_print:    graphs reused =          7\n",
            " 64%|██████▍   | 64/100 [14:06<06:49, 11.37s/it]Llama.generate: 35 prefix-match hit, remaining 1298 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =   20131.03 ms /  1298 tokens (   15.51 ms per token,    64.48 tokens per second)\n",
            "llama_perf_context_print:        eval time =   20666.88 ms /    99 runs   (  208.76 ms per token,     4.79 tokens per second)\n",
            "llama_perf_context_print:       total time =   41057.00 ms /  1397 tokens\n",
            "llama_perf_context_print:    graphs reused =         95\n",
            " 65%|██████▌   | 65/100 [14:47<11:49, 20.28s/it]Llama.generate: 35 prefix-match hit, remaining 197 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2651.28 ms /   197 tokens (   13.46 ms per token,    74.30 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1557.37 ms /     8 runs   (  194.67 ms per token,     5.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    4232.22 ms /   205 tokens\n",
            "llama_perf_context_print:    graphs reused =          7\n",
            " 66%|██████▌   | 66/100 [14:52<08:45, 15.47s/it]Llama.generate: 35 prefix-match hit, remaining 308 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    4117.87 ms /   308 tokens (   13.37 ms per token,    74.80 tokens per second)\n",
            "llama_perf_context_print:        eval time =   19656.53 ms /    99 runs   (  198.55 ms per token,     5.04 tokens per second)\n",
            "llama_perf_context_print:       total time =   24033.30 ms /   407 tokens\n",
            "llama_perf_context_print:    graphs reused =         95\n",
            " 67%|██████▋   | 67/100 [15:16<09:55, 18.04s/it]Llama.generate: 35 prefix-match hit, remaining 227 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3122.01 ms /   227 tokens (   13.75 ms per token,    72.71 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1621.97 ms /     8 runs   (  202.75 ms per token,     4.93 tokens per second)\n",
            "llama_perf_context_print:       total time =    4767.56 ms /   235 tokens\n",
            "llama_perf_context_print:    graphs reused =          7\n",
            " 68%|██████▊   | 68/100 [15:20<07:29, 14.06s/it]Llama.generate: 35 prefix-match hit, remaining 385 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    5130.98 ms /   385 tokens (   13.33 ms per token,    75.03 tokens per second)\n",
            "llama_perf_context_print:        eval time =   16534.28 ms /    83 runs   (  199.21 ms per token,     5.02 tokens per second)\n",
            "llama_perf_context_print:       total time =   21881.61 ms /   468 tokens\n",
            "llama_perf_context_print:    graphs reused =         80\n",
            " 69%|██████▉   | 69/100 [15:42<08:28, 16.41s/it]Llama.generate: 35 prefix-match hit, remaining 173 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2822.61 ms /   173 tokens (   16.32 ms per token,    61.29 tokens per second)\n",
            "llama_perf_context_print:        eval time =   14359.53 ms /    73 runs   (  196.71 ms per token,     5.08 tokens per second)\n",
            "llama_perf_context_print:       total time =   17372.01 ms /   246 tokens\n",
            "llama_perf_context_print:    graphs reused =         70\n",
            " 70%|███████   | 70/100 [16:00<08:20, 16.70s/it]Llama.generate: 35 prefix-match hit, remaining 190 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2599.14 ms /   190 tokens (   13.68 ms per token,    73.10 tokens per second)\n",
            "llama_perf_context_print:        eval time =   12691.72 ms /    64 runs   (  198.31 ms per token,     5.04 tokens per second)\n",
            "llama_perf_context_print:       total time =   15456.81 ms /   254 tokens\n",
            "llama_perf_context_print:    graphs reused =         61\n",
            " 71%|███████   | 71/100 [16:15<07:53, 16.33s/it]Llama.generate: 35 prefix-match hit, remaining 278 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    4925.92 ms /   278 tokens (   17.72 ms per token,    56.44 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1593.51 ms /     8 runs   (  199.19 ms per token,     5.02 tokens per second)\n",
            "llama_perf_context_print:       total time =    6542.78 ms /   286 tokens\n",
            "llama_perf_context_print:    graphs reused =          6\n",
            " 72%|███████▏  | 72/100 [16:22<06:15, 13.39s/it]Llama.generate: 35 prefix-match hit, remaining 174 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2425.38 ms /   174 tokens (   13.94 ms per token,    71.74 tokens per second)\n",
            "llama_perf_context_print:        eval time =    8225.48 ms /    42 runs   (  195.84 ms per token,     5.11 tokens per second)\n",
            "llama_perf_context_print:       total time =   10759.79 ms /   216 tokens\n",
            "llama_perf_context_print:    graphs reused =         40\n",
            " 73%|███████▎  | 73/100 [16:33<05:40, 12.61s/it]Llama.generate: 35 prefix-match hit, remaining 454 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    6057.03 ms /   454 tokens (   13.34 ms per token,    74.95 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1600.13 ms /     8 runs   (  200.02 ms per token,     5.00 tokens per second)\n",
            "llama_perf_context_print:       total time =    7680.67 ms /   462 tokens\n",
            "llama_perf_context_print:    graphs reused =          7\n",
            " 74%|███████▍  | 74/100 [16:40<04:49, 11.13s/it]Llama.generate: 35 prefix-match hit, remaining 150 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3362.02 ms /   150 tokens (   22.41 ms per token,    44.62 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1567.68 ms /     8 runs   (  195.96 ms per token,     5.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    4952.97 ms /   158 tokens\n",
            "llama_perf_context_print:    graphs reused =          6\n",
            " 75%|███████▌  | 75/100 [16:45<03:51,  9.28s/it]Llama.generate: 35 prefix-match hit, remaining 480 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    6325.62 ms /   480 tokens (   13.18 ms per token,    75.88 tokens per second)\n",
            "llama_perf_context_print:        eval time =   19844.56 ms /    99 runs   (  200.45 ms per token,     4.99 tokens per second)\n",
            "llama_perf_context_print:       total time =   26429.37 ms /   579 tokens\n",
            "llama_perf_context_print:    graphs reused =         95\n",
            " 76%|███████▌  | 76/100 [17:12<05:46, 14.43s/it]Llama.generate: 35 prefix-match hit, remaining 254 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3438.51 ms /   254 tokens (   13.54 ms per token,    73.87 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1580.37 ms /     8 runs   (  197.55 ms per token,     5.06 tokens per second)\n",
            "llama_perf_context_print:       total time =    5042.25 ms /   262 tokens\n",
            "llama_perf_context_print:    graphs reused =          7\n",
            " 77%|███████▋  | 77/100 [17:17<04:27, 11.61s/it]Llama.generate: 35 prefix-match hit, remaining 256 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3592.72 ms /   256 tokens (   14.03 ms per token,    71.26 tokens per second)\n",
            "llama_perf_context_print:        eval time =   15807.22 ms /    80 runs   (  197.59 ms per token,     5.06 tokens per second)\n",
            "llama_perf_context_print:       total time =   19607.76 ms /   336 tokens\n",
            "llama_perf_context_print:    graphs reused =         77\n",
            " 78%|███████▊  | 78/100 [17:36<05:08, 14.01s/it]Llama.generate: 35 prefix-match hit, remaining 128 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    1679.57 ms /   128 tokens (   13.12 ms per token,    76.21 tokens per second)\n",
            "llama_perf_context_print:        eval time =   11136.90 ms /    57 runs   (  195.38 ms per token,     5.12 tokens per second)\n",
            "llama_perf_context_print:       total time =   12963.84 ms /   185 tokens\n",
            "llama_perf_context_print:    graphs reused =         55\n",
            " 79%|███████▉  | 79/100 [17:49<04:47, 13.70s/it]Llama.generate: 35 prefix-match hit, remaining 206 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    4140.25 ms /   206 tokens (   20.10 ms per token,    49.76 tokens per second)\n",
            "llama_perf_context_print:        eval time =   11438.74 ms /    58 runs   (  197.22 ms per token,     5.07 tokens per second)\n",
            "llama_perf_context_print:       total time =   15729.12 ms /   264 tokens\n",
            "llama_perf_context_print:    graphs reused =         55\n",
            " 80%|████████  | 80/100 [18:05<04:46, 14.31s/it]Llama.generate: 35 prefix-match hit, remaining 192 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2533.09 ms /   192 tokens (   13.19 ms per token,    75.80 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1558.73 ms /     8 runs   (  194.84 ms per token,     5.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    4114.90 ms /   200 tokens\n",
            "llama_perf_context_print:    graphs reused =          7\n",
            " 81%|████████  | 81/100 [18:09<03:33, 11.25s/it]Llama.generate: 35 prefix-match hit, remaining 297 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3925.64 ms /   297 tokens (   13.22 ms per token,    75.66 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1597.69 ms /     8 runs   (  199.71 ms per token,     5.01 tokens per second)\n",
            "llama_perf_context_print:       total time =    5546.63 ms /   305 tokens\n",
            "llama_perf_context_print:    graphs reused =          7\n",
            " 82%|████████▏ | 82/100 [18:15<02:51,  9.54s/it]Llama.generate: 35 prefix-match hit, remaining 332 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    4813.22 ms /   332 tokens (   14.50 ms per token,    68.98 tokens per second)\n",
            "llama_perf_context_print:        eval time =   13106.21 ms /    66 runs   (  198.58 ms per token,     5.04 tokens per second)\n",
            "llama_perf_context_print:       total time =   18090.83 ms /   398 tokens\n",
            "llama_perf_context_print:    graphs reused =         63\n",
            " 83%|████████▎ | 83/100 [18:33<03:25, 12.11s/it]Llama.generate: 35 prefix-match hit, remaining 190 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2588.63 ms /   190 tokens (   13.62 ms per token,    73.40 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1557.29 ms /     8 runs   (  194.66 ms per token,     5.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    4169.19 ms /   198 tokens\n",
            "llama_perf_context_print:    graphs reused =          7\n",
            " 84%|████████▍ | 84/100 [18:37<02:35,  9.73s/it]Llama.generate: 35 prefix-match hit, remaining 447 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    7068.71 ms /   447 tokens (   15.81 ms per token,    63.24 tokens per second)\n",
            "llama_perf_context_print:        eval time =   19030.23 ms /    95 runs   (  200.32 ms per token,     4.99 tokens per second)\n",
            "llama_perf_context_print:       total time =   26347.37 ms /   542 tokens\n",
            "llama_perf_context_print:    graphs reused =         91\n",
            " 85%|████████▌ | 85/100 [19:03<03:40, 14.72s/it]Llama.generate: 35 prefix-match hit, remaining 252 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3345.60 ms /   252 tokens (   13.28 ms per token,    75.32 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1570.29 ms /     8 runs   (  196.29 ms per token,     5.09 tokens per second)\n",
            "llama_perf_context_print:       total time =    4939.11 ms /   260 tokens\n",
            "llama_perf_context_print:    graphs reused =          6\n",
            " 86%|████████▌ | 86/100 [19:08<02:44, 11.78s/it]Llama.generate: 35 prefix-match hit, remaining 239 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3245.06 ms /   239 tokens (   13.58 ms per token,    73.65 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1606.16 ms /     8 runs   (  200.77 ms per token,     4.98 tokens per second)\n",
            "llama_perf_context_print:       total time =    4874.66 ms /   247 tokens\n",
            "llama_perf_context_print:    graphs reused =          7\n",
            " 87%|████████▋ | 87/100 [19:13<02:06,  9.71s/it]Llama.generate: 35 prefix-match hit, remaining 428 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    6357.53 ms /   428 tokens (   14.85 ms per token,    67.32 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1614.52 ms /     8 runs   (  201.81 ms per token,     4.96 tokens per second)\n",
            "llama_perf_context_print:       total time =    7995.43 ms /   436 tokens\n",
            "llama_perf_context_print:    graphs reused =          7\n",
            " 88%|████████▊ | 88/100 [19:21<01:50,  9.20s/it]Llama.generate: 35 prefix-match hit, remaining 199 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2742.35 ms /   199 tokens (   13.78 ms per token,    72.57 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1559.63 ms /     8 runs   (  194.95 ms per token,     5.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    4325.35 ms /   207 tokens\n",
            "llama_perf_context_print:    graphs reused =          7\n",
            " 89%|████████▉ | 89/100 [19:25<01:25,  7.74s/it]Llama.generate: 35 prefix-match hit, remaining 255 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3935.87 ms /   255 tokens (   15.43 ms per token,    64.79 tokens per second)\n",
            "llama_perf_context_print:        eval time =    8717.43 ms /    44 runs   (  198.12 ms per token,     5.05 tokens per second)\n",
            "llama_perf_context_print:       total time =   12767.56 ms /   299 tokens\n",
            "llama_perf_context_print:    graphs reused =         42\n",
            " 90%|█████████ | 90/100 [19:38<01:32,  9.25s/it]Llama.generate: 35 prefix-match hit, remaining 181 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2417.07 ms /   181 tokens (   13.35 ms per token,    74.88 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1575.03 ms /     8 runs   (  196.88 ms per token,     5.08 tokens per second)\n",
            "llama_perf_context_print:       total time =    4015.06 ms /   189 tokens\n",
            "llama_perf_context_print:    graphs reused =          7\n",
            " 91%|█████████ | 91/100 [19:42<01:09,  7.68s/it]Llama.generate: 35 prefix-match hit, remaining 286 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    3857.15 ms /   286 tokens (   13.49 ms per token,    74.15 tokens per second)\n",
            "llama_perf_context_print:        eval time =   15485.11 ms /    78 runs   (  198.53 ms per token,     5.04 tokens per second)\n",
            "llama_perf_context_print:       total time =   19545.19 ms /   364 tokens\n",
            "llama_perf_context_print:    graphs reused =         75\n",
            " 92%|█████████▏| 92/100 [20:02<01:29, 11.24s/it]Llama.generate: 35 prefix-match hit, remaining 328 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    4351.22 ms /   328 tokens (   13.27 ms per token,    75.38 tokens per second)\n",
            "llama_perf_context_print:        eval time =   16423.92 ms /    83 runs   (  197.88 ms per token,     5.05 tokens per second)\n",
            "llama_perf_context_print:       total time =   20991.14 ms /   411 tokens\n",
            "llama_perf_context_print:    graphs reused =         80\n",
            " 93%|█████████▎| 93/100 [20:23<01:39, 14.17s/it]Llama.generate: 35 prefix-match hit, remaining 298 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    4657.09 ms /   298 tokens (   15.63 ms per token,    63.99 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1604.19 ms /     8 runs   (  200.52 ms per token,     4.99 tokens per second)\n",
            "llama_perf_context_print:       total time =    6284.83 ms /   306 tokens\n",
            "llama_perf_context_print:    graphs reused =          7\n",
            " 94%|█████████▍| 94/100 [20:29<01:10, 11.80s/it]Llama.generate: 35 prefix-match hit, remaining 379 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    5098.78 ms /   379 tokens (   13.45 ms per token,    74.33 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1576.79 ms /     8 runs   (  197.10 ms per token,     5.07 tokens per second)\n",
            "llama_perf_context_print:       total time =    6699.01 ms /   387 tokens\n",
            "llama_perf_context_print:    graphs reused =          6\n",
            " 95%|█████████▌| 95/100 [20:36<00:51, 10.27s/it]Llama.generate: 35 prefix-match hit, remaining 336 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    4409.69 ms /   336 tokens (   13.12 ms per token,    76.20 tokens per second)\n",
            "llama_perf_context_print:        eval time =   13038.85 ms /    66 runs   (  197.56 ms per token,     5.06 tokens per second)\n",
            "llama_perf_context_print:       total time =   17620.81 ms /   402 tokens\n",
            "llama_perf_context_print:    graphs reused =         63\n",
            " 96%|█████████▌| 96/100 [20:53<00:49, 12.48s/it]Llama.generate: 35 prefix-match hit, remaining 197 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2666.25 ms /   197 tokens (   13.53 ms per token,    73.89 tokens per second)\n",
            "llama_perf_context_print:        eval time =   16954.69 ms /    86 runs   (  197.15 ms per token,     5.07 tokens per second)\n",
            "llama_perf_context_print:       total time =   19844.47 ms /   283 tokens\n",
            "llama_perf_context_print:    graphs reused =         83\n",
            " 97%|█████████▋| 97/100 [21:13<00:44, 14.69s/it]Llama.generate: 35 prefix-match hit, remaining 191 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2635.80 ms /   191 tokens (   13.80 ms per token,    72.46 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1549.05 ms /     8 runs   (  193.63 ms per token,     5.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    4207.83 ms /   199 tokens\n",
            "llama_perf_context_print:    graphs reused =          7\n",
            " 98%|█████████▊| 98/100 [21:17<00:23, 11.55s/it]Llama.generate: 35 prefix-match hit, remaining 195 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2707.83 ms /   195 tokens (   13.89 ms per token,    72.01 tokens per second)\n",
            "llama_perf_context_print:        eval time =   14306.57 ms /    73 runs   (  195.98 ms per token,     5.10 tokens per second)\n",
            "llama_perf_context_print:       total time =   17203.73 ms /   268 tokens\n",
            "llama_perf_context_print:    graphs reused =         70\n",
            " 99%|█████████▉| 99/100 [21:35<00:13, 13.25s/it]Llama.generate: 35 prefix-match hit, remaining 209 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3934.33 ms\n",
            "llama_perf_context_print: prompt eval time =    2764.36 ms /   209 tokens (   13.23 ms per token,    75.61 tokens per second)\n",
            "llama_perf_context_print:        eval time =   14314.47 ms /    73 runs   (  196.09 ms per token,     5.10 tokens per second)\n",
            "llama_perf_context_print:       total time =   17268.17 ms /   282 tokens\n",
            "llama_perf_context_print:    graphs reused =         70\n",
            "100%|██████████| 100/100 [21:52<00:00, 13.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for cot: 0.84\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Report your results\n",
        "\n",
        "Check these amazing plots & discussion."
      ],
      "metadata": {
        "id": "M2oy0mbcOuTr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i1vuAbOC4dew"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}
